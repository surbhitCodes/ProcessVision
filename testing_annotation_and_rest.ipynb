{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step1: Annotation",
   "id": "dd4af901bd63afd4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test1 - Full image classification",
   "id": "3e73b1caca349a90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T00:16:17.363530Z",
     "start_time": "2024-07-18T00:16:17.359289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# from detectron2.engine import DefaultPredictor\n",
    "# from detectron2.config import get_cfg\n",
    "# from research.detectron2.detectron2.model_zoo import model_zoo\n",
    "# from detectron2.utils.visualizer import Visualizer\n",
    "# from detectron2.data import MetadataCatalog\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import json\n",
    "# \n",
    "# # Configure the Detectron2 model\n",
    "# cfg = get_cfg()\n",
    "# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "# cfg.MODEL.DEVICE = \"cpu\"  # Ensure the model uses the CPU\n",
    "# \n",
    "# # Create a predictor\n",
    "# predictor = DefaultPredictor(cfg)\n",
    "# \n",
    "# # Provide the path to your input images\n",
    "# image_dir = \"./frames/\"  # Change this to the directory containing your images\n",
    "# output_json = \"annotations.json\"\n",
    "# \n",
    "# # Initialize annotation data\n",
    "# annotations = {\n",
    "#     \"images\": [],\n",
    "#     \"annotations\": [],\n",
    "#     \"categories\": [\n",
    "#         {\"id\": 1, \"name\": \"book\"},\n",
    "#         {\"id\": 2, \"name\": \"hand\"}\n",
    "#     ]\n",
    "# }\n",
    "# annotation_id = 1\n",
    "# \n",
    "# # Iterate over images in the directory\n",
    "# for image_file in os.listdir(image_dir):\n",
    "#     image_path = os.path.join(image_dir, image_file)\n",
    "# \n",
    "#     # Check if the file exists and is an image\n",
    "#     if not os.path.exists(image_path) or not image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "#         continue\n",
    "# \n",
    "#     # Read the image using OpenCV\n",
    "#     im = cv2.imread(image_path)\n",
    "# \n",
    "#     # Check if the image was successfully loaded\n",
    "#     if im is None:\n",
    "#         print(f\"Could not read the image {image_path}. Skipping.\")\n",
    "#         continue\n",
    "# \n",
    "#     # Perform inference\n",
    "#     outputs = predictor(im)\n",
    "# \n",
    "#     # Get bounding boxes and classes\n",
    "#     instances = outputs[\"instances\"].to(\"cpu\")\n",
    "#     boxes = instances.pred_boxes.tensor.numpy()\n",
    "#     classes = instances.pred_classes.numpy()\n",
    "# \n",
    "#     # Add image info to annotations\n",
    "#     image_info = {\n",
    "#         \"file_name\": image_file,\n",
    "#         \"height\": im.shape[0],\n",
    "#         \"width\": im.shape[1],\n",
    "#         \"id\": len(annotations[\"images\"]) + 1\n",
    "#     }\n",
    "#     annotations[\"images\"].append(image_info)\n",
    "# \n",
    "#     # Add annotations for this image\n",
    "#     for i, box in enumerate(boxes):\n",
    "#         category_id = classes[i] + 1  # Adjusting class id to start from 1 for COCO format\n",
    "#         annotation = {\n",
    "#             \"id\": annotation_id,\n",
    "#             \"image_id\": image_info[\"id\"],\n",
    "#             \"category_id\": category_id,\n",
    "#             \"bbox\": [float(box[0]), float(box[1]), float(box[2] - box[0]), float(box[3] - box[1])],\n",
    "#             \"area\": float((box[2] - box[0]) * (box[3] - box[1])),\n",
    "#             \"iscrowd\": 0\n",
    "#         }\n",
    "#         annotations[\"annotations\"].append(annotation_id)\n",
    "#         annotation_id += 1\n",
    "# \n",
    "# # Save annotations to a JSON file\n",
    "# with open(output_json, \"w\") as f:\n",
    "#     json.dump(annotations, f)\n",
    "# \n",
    "# print(f\"Annotations saved to {output_json}\")\n"
   ],
   "id": "ca627b080f82420d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test2 - bbox based",
   "id": "ee6deb4b14ce15bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T02:40:46.657850Z",
     "start_time": "2024-07-18T02:39:18.662390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from research.detectron2.detectron2 import model_zoo\n",
    "from detectron2.data import MetadataCatalog\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Configure the Detectron2 model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.MODEL.DEVICE = \"cpu\"  # Ensure the model uses the CPU\n",
    "\n",
    "# Create a predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# COCO metadata\n",
    "coco_metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
    "coco_classes = coco_metadata.thing_classes\n",
    "\n",
    "# Provide the path to your input images\n",
    "image_dir = \"./frames\"  # Change this to the directory containing your images\n",
    "output_json = \"annotations.json\"\n",
    "\n",
    "# Initialize annotation data\n",
    "annotations = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [\n",
    "        {\"id\": 1, \"name\": \"person\"},\n",
    "        {\"id\": 2, \"name\": \"book\"}\n",
    "    ]\n",
    "}\n",
    "annotation_id = 1\n",
    "\n",
    "# Iterate over images in the directory\n",
    "for image_file in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "\n",
    "    # Check if the file exists and is an image\n",
    "    if not os.path.exists(image_path) or not image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        continue\n",
    "\n",
    "    # Read the image using OpenCV\n",
    "    im = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was successfully loaded\n",
    "    if im is None:\n",
    "        print(f\"Could not read the image {image_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = predictor(im)\n",
    "\n",
    "    # Get bounding boxes and classes\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    boxes = instances.pred_boxes.tensor.numpy()\n",
    "    classes = instances.pred_classes.numpy()\n",
    "\n",
    "    # Add image info to annotations\n",
    "    image_info = {\n",
    "        \"file_name\": image_file,\n",
    "        \"height\": im.shape[0],\n",
    "        \"width\": im.shape[1],\n",
    "        \"id\": len(annotations[\"images\"]) + 1\n",
    "    }\n",
    "    annotations[\"images\"].append(image_info)\n",
    "\n",
    "    # Add annotations for this image\n",
    "    for i, box in enumerate(boxes):\n",
    "        class_name = coco_classes[classes[i]]\n",
    "        if class_name == \"person\":\n",
    "            category_id = 0\n",
    "        elif class_name == \"book\":\n",
    "            category_id = 1\n",
    "        else:\n",
    "            continue  # Skip other classes\n",
    "\n",
    "        annotation = {\n",
    "            \"id\": annotation_id,\n",
    "            \"image_id\": image_info[\"id\"],\n",
    "            \"category_id\": category_id,\n",
    "            \"bbox\": [float(box[0]), float(box[1]), float(box[2] - box[0]), float(box[3] - box[1])],\n",
    "            \"area\": float((box[2] - box[0]) * (box[3] - box[1])),\n",
    "            \"iscrowd\": 0\n",
    "        }\n",
    "        annotations[\"annotations\"].append(annotation)\n",
    "        annotation_id += 1\n",
    "\n",
    "# Save annotations to a JSON file\n",
    "with open(output_json, \"w\") as f:\n",
    "    json.dump(annotations, f)\n",
    "\n",
    "print(f\"Annotations saved to {output_json}\")\n"
   ],
   "id": "6317d83c1b1f3072",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[07/17 22:39:19 d2.checkpoint.detection_checkpoint]: \u001B[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n",
      "Annotations saved to annotations.json\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step2: Training the model",
   "id": "321be56aaf07e476"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Dataset preperation and training",
   "id": "7d5a52d7ab03d590"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T03:46:14.052690Z",
     "start_time": "2024-07-18T03:46:14.050134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# Unregister the datasets if they are already registered\n",
    "for d in [\"train\", \"val\"]:\n",
    "    dataset_name = \"custom_\" + d\n",
    "    if dataset_name in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(dataset_name)\n",
    "    if dataset_name in MetadataCatalog.list():\n",
    "        MetadataCatalog.remove(dataset_name)\n"
   ],
   "id": "c73258a236f3bde6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T08:21:00.591970Z",
     "start_time": "2024-07-18T03:46:16.653900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from research.detectron2.detectron2 import model_zoo\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "annotations_file = 'annotations.json'\n",
    "image_root = './frames'\n",
    "\n",
    "def get_custom_dicts():\n",
    "    with open(annotations_file) as f:\n",
    "        dataset_dicts = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "    for image_info in dataset_dicts['images']:\n",
    "        record = {}\n",
    "        record[\"file_name\"] = os.path.join(image_root, image_info[\"file_name\"])\n",
    "        record[\"image_id\"] = image_info[\"id\"]\n",
    "        record[\"height\"] = image_info[\"height\"]\n",
    "        record[\"width\"] = image_info[\"width\"]\n",
    "\n",
    "        record[\"annotations\"] = []\n",
    "        for anno in dataset_dicts['annotations']:\n",
    "            if anno[\"image_id\"] == image_info[\"id\"]:\n",
    "                obj = {\n",
    "                    \"bbox\": anno[\"bbox\"],\n",
    "                    \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                    \"category_id\": anno[\"category_id\"],\n",
    "                    \"iscrowd\": anno[\"iscrowd\"]\n",
    "                }\n",
    "                record[\"annotations\"].append(obj)\n",
    "        dataset.append(record)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Register the datasets\n",
    "for d in [\"train\", \"val\"]:\n",
    "    DatasetCatalog.register(\"custom_\" + d, get_custom_dicts)\n",
    "    MetadataCatalog.get(\"custom_\" + d).set(thing_classes=[\"person\", \"book\"])\n",
    "custom_metadata = MetadataCatalog.get(\"custom_train\")\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"custom_train\",)\n",
    "cfg.DATASETS.TEST = (\"custom_val\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 1000\n",
    "cfg.SOLVER.STEPS = (700, 900)\n",
    "cfg.SOLVER.GAMMA = 0.1\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # person and book\n",
    "cfg.MODEL.DEVICE = \"cpu\"  # Use CPU for training\n",
    "cfg.TEST.EVAL_PERIOD = 50  # Evaluate every 50 iterations\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Implementing a custom trainer to include evaluation\n",
    "class TrainerWithVal(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "        self._last_eval_results = None\n",
    "\n",
    "    def run_step(self):\n",
    "        self._trainer.iter = self.iter\n",
    "        self._trainer.run_step()\n",
    "        if (self.iter + 1) % cfg.TEST.EVAL_PERIOD == 0:\n",
    "            self._last_eval_results = self.test(self.cfg, self.model)\n",
    "            comm.synchronize()\n",
    "\n",
    "    def test(self, cfg, model, evaluators=None):\n",
    "        if evaluators is None:\n",
    "            evaluators = [self.build_evaluator(cfg, name) for name in cfg.DATASETS.TEST]\n",
    "        res = inference_on_dataset(model, build_detection_test_loader(cfg, cfg.DATASETS.TEST[0]), evaluators[0])\n",
    "        return res\n",
    "\n",
    "trainer = TrainerWithVal(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n"
   ],
   "id": "87fffefb67a02b37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[07/17 23:46:17 d2.engine.defaults]: \u001B[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001B[32m[07/17 23:46:17 d2.data.build]: \u001B[0mRemoved 0 images with no usable annotations. 79 images left.\n",
      "\u001B[32m[07/17 23:46:17 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001B[32m[07/17 23:46:17 d2.data.build]: \u001B[0mUsing training sampler TrainingSampler\n",
      "\u001B[32m[07/17 23:46:17 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/17 23:46:17 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/17 23:46:17 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/17 23:46:17 d2.data.build]: \u001B[0mMaking batched data loader with batch_size=2\n",
      "\u001B[32m[07/17 23:46:17 d2.checkpoint.detection_checkpoint]: \u001B[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001B[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001B[0m\n",
      "\u001B[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[07/17 23:46:17 d2.engine.train_loop]: \u001B[0mStarting training from iteration 0\n",
      "\u001B[32m[07/17 23:47:42 d2.utils.events]: \u001B[0m eta: 1:08:08  iter: 19  total_loss: 2.153  loss_cls: 1.016  loss_box_reg: 0.9059  loss_rpn_cls: 0.01852  loss_rpn_loc: 0.1921    time: 4.1128  last_time: 4.1161  data_time: 0.1002  last_data_time: 0.0011   lr: 2.9275e-07  \n",
      "\u001B[32m[07/17 23:49:09 d2.utils.events]: \u001B[0m eta: 1:08:20  iter: 39  total_loss: 2.133  loss_cls: 1.013  loss_box_reg: 0.8915  loss_rpn_cls: 0.02005  loss_rpn_loc: 0.1859    time: 4.2371  last_time: 3.6182  data_time: 0.0010  last_data_time: 0.0009   lr: 3.3775e-07  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/17 23:49:52 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/17 23:49:52 d2.evaluation.coco_evaluation]: \u001B[0mTrying to convert 'custom_val' to COCO format ...\n",
      "\u001B[32m[07/17 23:49:52 d2.data.datasets.coco]: \u001B[0mConverting annotations of dataset 'custom_val' to COCO format ...)\n",
      "\u001B[32m[07/17 23:49:52 d2.data.datasets.coco]: \u001B[0mConverting dataset dicts into COCO format\n",
      "\u001B[32m[07/17 23:49:52 d2.data.datasets.coco]: \u001B[0mConversion finished, #images: 79, #annotations: 1692\n",
      "\u001B[32m[07/17 23:49:52 d2.data.datasets.coco]: \u001B[0mCaching COCO format annotations at './output/inference/custom_val_coco_format.json' ...\n",
      "\u001B[32m[07/17 23:49:52 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/17 23:49:52 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/17 23:49:52 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/17 23:49:52 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/17 23:49:52 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/17 23:50:04 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 0.9176 s/iter. Eval: 0.0001 s/iter. Total: 0.9180 s/iter. ETA=0:01:02\n",
      "\u001B[32m[07/17 23:50:10 d2.evaluation.evaluator]: \u001B[0mInference done 17/79. Dataloading: 0.0003 s/iter. Inference: 0.9000 s/iter. Eval: 0.0001 s/iter. Total: 0.9005 s/iter. ETA=0:00:55\n",
      "\u001B[32m[07/17 23:50:15 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0003 s/iter. Inference: 0.9007 s/iter. Eval: 0.0001 s/iter. Total: 0.9013 s/iter. ETA=0:00:50\n",
      "\u001B[32m[07/17 23:50:21 d2.evaluation.evaluator]: \u001B[0mInference done 29/79. Dataloading: 0.0004 s/iter. Inference: 0.9064 s/iter. Eval: 0.0001 s/iter. Total: 0.9070 s/iter. ETA=0:00:45\n",
      "\u001B[32m[07/17 23:50:26 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0004 s/iter. Inference: 0.9004 s/iter. Eval: 0.0001 s/iter. Total: 0.9010 s/iter. ETA=0:00:39\n",
      "\u001B[32m[07/17 23:50:31 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 0.8982 s/iter. Eval: 0.0001 s/iter. Total: 0.8988 s/iter. ETA=0:00:34\n",
      "\u001B[32m[07/17 23:50:37 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0004 s/iter. Inference: 0.9035 s/iter. Eval: 0.0001 s/iter. Total: 0.9041 s/iter. ETA=0:00:28\n",
      "\u001B[32m[07/17 23:50:42 d2.evaluation.evaluator]: \u001B[0mInference done 52/79. Dataloading: 0.0004 s/iter. Inference: 0.9203 s/iter. Eval: 0.0001 s/iter. Total: 0.9209 s/iter. ETA=0:00:24\n",
      "\u001B[32m[07/17 23:50:47 d2.evaluation.evaluator]: \u001B[0mInference done 58/79. Dataloading: 0.0004 s/iter. Inference: 0.9187 s/iter. Eval: 0.0001 s/iter. Total: 0.9194 s/iter. ETA=0:00:19\n",
      "\u001B[32m[07/17 23:50:53 d2.evaluation.evaluator]: \u001B[0mInference done 64/79. Dataloading: 0.0004 s/iter. Inference: 0.9136 s/iter. Eval: 0.0001 s/iter. Total: 0.9142 s/iter. ETA=0:00:13\n",
      "\u001B[32m[07/17 23:50:58 d2.evaluation.evaluator]: \u001B[0mInference done 70/79. Dataloading: 0.0004 s/iter. Inference: 0.9102 s/iter. Eval: 0.0001 s/iter. Total: 0.9108 s/iter. ETA=0:00:08\n",
      "\u001B[32m[07/17 23:51:03 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0004 s/iter. Inference: 0.9086 s/iter. Eval: 0.0001 s/iter. Total: 0.9092 s/iter. ETA=0:00:02\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:07.581886 (0.913269 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:07 (0.908653 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.007\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.049\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.049\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.232 | 1.094  | 0.012  |  nan  |  nan  | 0.278 |\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.005 | book       | 0.459 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/17 23:51:06 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/17 23:51:06 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/17 23:51:06 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/17 23:51:06 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/17 23:51:06 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/17 23:51:06 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/17 23:51:19 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 0.9062 s/iter. Eval: 0.0001 s/iter. Total: 0.9066 s/iter. ETA=0:01:01\n",
      "\u001B[32m[07/17 23:51:24 d2.evaluation.evaluator]: \u001B[0mInference done 17/79. Dataloading: 0.0003 s/iter. Inference: 0.8993 s/iter. Eval: 0.0001 s/iter. Total: 0.8998 s/iter. ETA=0:00:55\n",
      "\u001B[32m[07/17 23:51:30 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0003 s/iter. Inference: 0.8998 s/iter. Eval: 0.0001 s/iter. Total: 0.9004 s/iter. ETA=0:00:50\n",
      "\u001B[32m[07/17 23:51:35 d2.evaluation.evaluator]: \u001B[0mInference done 29/79. Dataloading: 0.0004 s/iter. Inference: 0.9039 s/iter. Eval: 0.0001 s/iter. Total: 0.9045 s/iter. ETA=0:00:45\n",
      "\u001B[32m[07/17 23:51:41 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0004 s/iter. Inference: 0.9038 s/iter. Eval: 0.0001 s/iter. Total: 0.9044 s/iter. ETA=0:00:39\n",
      "\u001B[32m[07/17 23:51:46 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 0.9105 s/iter. Eval: 0.0001 s/iter. Total: 0.9111 s/iter. ETA=0:00:34\n",
      "\u001B[32m[07/17 23:51:52 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0004 s/iter. Inference: 0.9097 s/iter. Eval: 0.0001 s/iter. Total: 0.9103 s/iter. ETA=0:00:29\n",
      "\u001B[32m[07/17 23:51:57 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0004 s/iter. Inference: 0.9048 s/iter. Eval: 0.0001 s/iter. Total: 0.9054 s/iter. ETA=0:00:23\n",
      "\u001B[32m[07/17 23:52:02 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0004 s/iter. Inference: 0.9018 s/iter. Eval: 0.0001 s/iter. Total: 0.9024 s/iter. ETA=0:00:18\n",
      "\u001B[32m[07/17 23:52:07 d2.evaluation.evaluator]: \u001B[0mInference done 65/79. Dataloading: 0.0004 s/iter. Inference: 0.8980 s/iter. Eval: 0.0001 s/iter. Total: 0.8986 s/iter. ETA=0:00:12\n",
      "\u001B[32m[07/17 23:52:13 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0004 s/iter. Inference: 0.8972 s/iter. Eval: 0.0001 s/iter. Total: 0.8979 s/iter. ETA=0:00:07\n",
      "\u001B[32m[07/17 23:52:18 d2.evaluation.evaluator]: \u001B[0mInference done 77/79. Dataloading: 0.0004 s/iter. Inference: 0.8969 s/iter. Eval: 0.0001 s/iter. Total: 0.8975 s/iter. ETA=0:00:01\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:06.672068 (0.900974 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:06 (0.896457 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.14s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.007\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.049\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.049\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.232 | 1.094  | 0.012  |  nan  |  nan  | 0.278 |\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/17 23:52:20 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.005 | book       | 0.459 |\n",
      "\u001B[32m[07/17 23:53:03 d2.utils.events]: \u001B[0m eta: 1:08:11  iter: 59  total_loss: 2.084  loss_cls: 1.007  loss_box_reg: 0.8696  loss_rpn_cls: 0.02003  loss_rpn_loc: 0.1794    time: 5.5462  last_time: 4.4819  data_time: 0.0010  last_data_time: 0.0008   lr: 3.8275e-07  \n",
      "\u001B[32m[07/17 23:54:28 d2.utils.events]: \u001B[0m eta: 1:06:32  iter: 79  total_loss: 2.152  loss_cls: 1.005  loss_box_reg: 0.9251  loss_rpn_cls: 0.02016  loss_rpn_loc: 0.1953    time: 5.2046  last_time: 4.8708  data_time: 0.0010  last_data_time: 0.0009   lr: 4.2775e-07  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/17 23:55:51 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/17 23:55:51 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/17 23:55:51 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/17 23:55:51 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/17 23:55:51 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/17 23:55:51 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/17 23:56:03 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 0.9313 s/iter. Eval: 0.0001 s/iter. Total: 0.9317 s/iter. ETA=0:01:03\n",
      "\u001B[32m[07/17 23:56:09 d2.evaluation.evaluator]: \u001B[0mInference done 17/79. Dataloading: 0.0003 s/iter. Inference: 0.9298 s/iter. Eval: 0.0001 s/iter. Total: 0.9304 s/iter. ETA=0:00:57\n",
      "\u001B[32m[07/17 23:56:14 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0003 s/iter. Inference: 0.9416 s/iter. Eval: 0.0001 s/iter. Total: 0.9421 s/iter. ETA=0:00:52\n",
      "\u001B[32m[07/17 23:56:20 d2.evaluation.evaluator]: \u001B[0mInference done 29/79. Dataloading: 0.0003 s/iter. Inference: 0.9486 s/iter. Eval: 0.0001 s/iter. Total: 0.9492 s/iter. ETA=0:00:47\n",
      "\u001B[32m[07/17 23:56:26 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0004 s/iter. Inference: 0.9488 s/iter. Eval: 0.0001 s/iter. Total: 0.9493 s/iter. ETA=0:00:41\n",
      "\u001B[32m[07/17 23:56:32 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 0.9487 s/iter. Eval: 0.0001 s/iter. Total: 0.9493 s/iter. ETA=0:00:36\n",
      "\u001B[32m[07/17 23:56:37 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0004 s/iter. Inference: 0.9456 s/iter. Eval: 0.0001 s/iter. Total: 0.9462 s/iter. ETA=0:00:30\n",
      "\u001B[32m[07/17 23:56:43 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0004 s/iter. Inference: 0.9434 s/iter. Eval: 0.0001 s/iter. Total: 0.9440 s/iter. ETA=0:00:24\n",
      "\u001B[32m[07/17 23:56:48 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0004 s/iter. Inference: 0.9409 s/iter. Eval: 0.0001 s/iter. Total: 0.9415 s/iter. ETA=0:00:18\n",
      "\u001B[32m[07/17 23:56:54 d2.evaluation.evaluator]: \u001B[0mInference done 65/79. Dataloading: 0.0004 s/iter. Inference: 0.9386 s/iter. Eval: 0.0001 s/iter. Total: 0.9392 s/iter. ETA=0:00:13\n",
      "\u001B[32m[07/17 23:56:59 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0004 s/iter. Inference: 0.9377 s/iter. Eval: 0.0001 s/iter. Total: 0.9383 s/iter. ETA=0:00:07\n",
      "\u001B[32m[07/17 23:57:05 d2.evaluation.evaluator]: \u001B[0mInference done 77/79. Dataloading: 0.0004 s/iter. Inference: 0.9366 s/iter. Eval: 0.0001 s/iter. Total: 0.9372 s/iter. ETA=0:00:01\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:09.570635 (0.940144 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:09 (0.935762 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.007\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.053\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.053\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.272 | 1.278  | 0.012  |  nan  |  nan  | 0.323 |\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.006 | book       | 0.538 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/17 23:57:07 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/17 23:57:07 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/17 23:57:07 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/17 23:57:07 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/17 23:57:07 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/17 23:57:07 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/17 23:57:20 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 0.9672 s/iter. Eval: 0.0002 s/iter. Total: 0.9677 s/iter. ETA=0:01:05\n",
      "\u001B[32m[07/17 23:57:25 d2.evaluation.evaluator]: \u001B[0mInference done 17/79. Dataloading: 0.0003 s/iter. Inference: 0.9469 s/iter. Eval: 0.0002 s/iter. Total: 0.9475 s/iter. ETA=0:00:58\n",
      "\u001B[32m[07/17 23:57:31 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0003 s/iter. Inference: 0.9349 s/iter. Eval: 0.0001 s/iter. Total: 0.9355 s/iter. ETA=0:00:52\n",
      "\u001B[32m[07/17 23:57:36 d2.evaluation.evaluator]: \u001B[0mInference done 29/79. Dataloading: 0.0003 s/iter. Inference: 0.9283 s/iter. Eval: 0.0001 s/iter. Total: 0.9288 s/iter. ETA=0:00:46\n",
      "\u001B[32m[07/17 23:57:42 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0003 s/iter. Inference: 0.9247 s/iter. Eval: 0.0001 s/iter. Total: 0.9252 s/iter. ETA=0:00:40\n",
      "\u001B[32m[07/17 23:57:47 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0003 s/iter. Inference: 0.9246 s/iter. Eval: 0.0001 s/iter. Total: 0.9252 s/iter. ETA=0:00:35\n",
      "\u001B[32m[07/17 23:57:53 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0003 s/iter. Inference: 0.9266 s/iter. Eval: 0.0001 s/iter. Total: 0.9272 s/iter. ETA=0:00:29\n",
      "\u001B[32m[07/17 23:57:58 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0003 s/iter. Inference: 0.9264 s/iter. Eval: 0.0001 s/iter. Total: 0.9270 s/iter. ETA=0:00:24\n",
      "\u001B[32m[07/17 23:58:04 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0003 s/iter. Inference: 0.9274 s/iter. Eval: 0.0001 s/iter. Total: 0.9280 s/iter. ETA=0:00:18\n",
      "\u001B[32m[07/17 23:58:09 d2.evaluation.evaluator]: \u001B[0mInference done 65/79. Dataloading: 0.0003 s/iter. Inference: 0.9251 s/iter. Eval: 0.0001 s/iter. Total: 0.9257 s/iter. ETA=0:00:12\n",
      "\u001B[32m[07/17 23:58:15 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0003 s/iter. Inference: 0.9242 s/iter. Eval: 0.0001 s/iter. Total: 0.9248 s/iter. ETA=0:00:07\n",
      "\u001B[32m[07/17 23:58:20 d2.evaluation.evaluator]: \u001B[0mInference done 77/79. Dataloading: 0.0004 s/iter. Inference: 0.9234 s/iter. Eval: 0.0001 s/iter. Total: 0.9240 s/iter. ETA=0:00:01\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:08.650256 (0.927706 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:08 (0.923101 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.003\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.007\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.053\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.053\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.272 | 1.278  | 0.012  |  nan  |  nan  | 0.323 |\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/17 23:58:23 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.006 | book       | 0.538 |\n",
      "\u001B[32m[07/17 23:58:23 d2.utils.events]: \u001B[0m eta: 1:04:46  iter: 99  total_loss: 2.051  loss_cls: 0.9993  loss_box_reg: 0.8415  loss_rpn_cls: 0.02388  loss_rpn_loc: 0.1837    time: 5.7692  last_time: 79.7073  data_time: 0.0010  last_data_time: 0.0010   lr: 4.7275e-07  \n",
      "\u001B[32m[07/17 23:59:47 d2.utils.events]: \u001B[0m eta: 1:03:16  iter: 119  total_loss: 2.09  loss_cls: 0.9969  loss_box_reg: 0.8733  loss_rpn_cls: 0.02284  loss_rpn_loc: 0.1879    time: 5.5092  last_time: 4.6769  data_time: 0.0010  last_data_time: 0.0010   lr: 5.1775e-07  \n",
      "\u001B[32m[07/18 00:01:12 d2.utils.events]: \u001B[0m eta: 1:01:50  iter: 139  total_loss: 2.058  loss_cls: 0.9892  loss_box_reg: 0.8656  loss_rpn_cls: 0.01686  loss_rpn_loc: 0.1843    time: 5.3240  last_time: 4.1294  data_time: 0.0010  last_data_time: 0.0010   lr: 5.6275e-07  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:01:55 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:01:55 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:01:55 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:01:55 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:01:55 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:01:55 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:02:07 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 0.9334 s/iter. Eval: 0.0002 s/iter. Total: 0.9339 s/iter. ETA=0:01:03\n",
      "\u001B[32m[07/18 00:02:13 d2.evaluation.evaluator]: \u001B[0mInference done 17/79. Dataloading: 0.0003 s/iter. Inference: 0.9218 s/iter. Eval: 0.0002 s/iter. Total: 0.9223 s/iter. ETA=0:00:57\n",
      "\u001B[32m[07/18 00:02:18 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0003 s/iter. Inference: 0.9251 s/iter. Eval: 0.0002 s/iter. Total: 0.9257 s/iter. ETA=0:00:51\n",
      "\u001B[32m[07/18 00:02:24 d2.evaluation.evaluator]: \u001B[0mInference done 29/79. Dataloading: 0.0003 s/iter. Inference: 0.9380 s/iter. Eval: 0.0001 s/iter. Total: 0.9386 s/iter. ETA=0:00:46\n",
      "\u001B[32m[07/18 00:02:30 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0003 s/iter. Inference: 0.9386 s/iter. Eval: 0.0001 s/iter. Total: 0.9392 s/iter. ETA=0:00:41\n",
      "\u001B[32m[07/18 00:02:36 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0003 s/iter. Inference: 0.9396 s/iter. Eval: 0.0001 s/iter. Total: 0.9402 s/iter. ETA=0:00:35\n",
      "\u001B[32m[07/18 00:02:41 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0003 s/iter. Inference: 0.9401 s/iter. Eval: 0.0001 s/iter. Total: 0.9407 s/iter. ETA=0:00:30\n",
      "\u001B[32m[07/18 00:02:47 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0004 s/iter. Inference: 0.9448 s/iter. Eval: 0.0001 s/iter. Total: 0.9454 s/iter. ETA=0:00:24\n",
      "\u001B[32m[07/18 00:02:53 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0004 s/iter. Inference: 0.9434 s/iter. Eval: 0.0001 s/iter. Total: 0.9440 s/iter. ETA=0:00:18\n",
      "\u001B[32m[07/18 00:02:58 d2.evaluation.evaluator]: \u001B[0mInference done 65/79. Dataloading: 0.0004 s/iter. Inference: 0.9416 s/iter. Eval: 0.0001 s/iter. Total: 0.9422 s/iter. ETA=0:00:13\n",
      "\u001B[32m[07/18 00:03:04 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0004 s/iter. Inference: 0.9388 s/iter. Eval: 0.0001 s/iter. Total: 0.9394 s/iter. ETA=0:00:07\n",
      "\u001B[32m[07/18 00:03:10 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0004 s/iter. Inference: 0.9536 s/iter. Eval: 0.0001 s/iter. Total: 0.9542 s/iter. ETA=0:00:02\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:12.352416 (0.977735 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:11 (0.972730 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.008\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.090\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.090\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.336 | 1.554  | 0.015  |  nan  |  nan  | 0.396 |\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.010 | book       | 0.662 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:03:14 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:03:14 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:03:14 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:03:14 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:03:14 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:03:14 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:03:29 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 0.9158 s/iter. Eval: 0.0001 s/iter. Total: 0.9162 s/iter. ETA=0:01:02\n",
      "\u001B[32m[07/18 00:03:34 d2.evaluation.evaluator]: \u001B[0mInference done 17/79. Dataloading: 0.0003 s/iter. Inference: 0.9094 s/iter. Eval: 0.0072 s/iter. Total: 0.9170 s/iter. ETA=0:00:56\n",
      "\u001B[32m[07/18 00:03:40 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0003 s/iter. Inference: 0.9073 s/iter. Eval: 0.0048 s/iter. Total: 0.9126 s/iter. ETA=0:00:51\n",
      "\u001B[32m[07/18 00:03:45 d2.evaluation.evaluator]: \u001B[0mInference done 29/79. Dataloading: 0.0003 s/iter. Inference: 0.9042 s/iter. Eval: 0.0037 s/iter. Total: 0.9083 s/iter. ETA=0:00:45\n",
      "\u001B[32m[07/18 00:03:51 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0003 s/iter. Inference: 0.9044 s/iter. Eval: 0.0030 s/iter. Total: 0.9078 s/iter. ETA=0:00:39\n",
      "\u001B[32m[07/18 00:03:56 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0003 s/iter. Inference: 0.9054 s/iter. Eval: 0.0025 s/iter. Total: 0.9083 s/iter. ETA=0:00:34\n",
      "\u001B[32m[07/18 00:04:01 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0003 s/iter. Inference: 0.9046 s/iter. Eval: 0.0021 s/iter. Total: 0.9072 s/iter. ETA=0:00:29\n",
      "\u001B[32m[07/18 00:04:07 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0003 s/iter. Inference: 0.9048 s/iter. Eval: 0.0019 s/iter. Total: 0.9072 s/iter. ETA=0:00:23\n",
      "\u001B[32m[07/18 00:04:12 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0003 s/iter. Inference: 0.9050 s/iter. Eval: 0.0017 s/iter. Total: 0.9072 s/iter. ETA=0:00:18\n",
      "\u001B[32m[07/18 00:04:18 d2.evaluation.evaluator]: \u001B[0mInference done 65/79. Dataloading: 0.0003 s/iter. Inference: 0.9047 s/iter. Eval: 0.0015 s/iter. Total: 0.9067 s/iter. ETA=0:00:12\n",
      "\u001B[32m[07/18 00:04:23 d2.evaluation.evaluator]: \u001B[0mInference done 69/79. Dataloading: 0.0004 s/iter. Inference: 0.9283 s/iter. Eval: 0.0015 s/iter. Total: 0.9302 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 00:04:29 d2.evaluation.evaluator]: \u001B[0mInference done 74/79. Dataloading: 0.0004 s/iter. Inference: 0.9498 s/iter. Eval: 0.0014 s/iter. Total: 0.9516 s/iter. ETA=0:00:04\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.evaluator]: \u001B[0mInference done 79/79. Dataloading: 0.0004 s/iter. Inference: 0.9658 s/iter. Eval: 0.0013 s/iter. Total: 0.9676 s/iter. ETA=0:00:00\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:12.060183 (0.973786 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:11 (0.965835 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.008\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.090\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.090\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.336 | 1.554  | 0.015  |  nan  |  nan  | 0.396 |\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:04:35 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.010 | book       | 0.662 |\n",
      "\u001B[32m[07/18 00:05:29 d2.utils.events]: \u001B[0m eta: 1:00:49  iter: 159  total_loss: 2.071  loss_cls: 0.9826  loss_box_reg: 0.8903  loss_rpn_cls: 0.02137  loss_rpn_loc: 0.1836    time: 5.7610  last_time: 5.0744  data_time: 0.0011  last_data_time: 0.0014   lr: 6.0775e-07  \n",
      "\u001B[32m[07/18 00:07:13 d2.utils.events]: \u001B[0m eta: 1:00:18  iter: 179  total_loss: 2.08  loss_cls: 0.9746  loss_box_reg: 0.8825  loss_rpn_cls: 0.02742  loss_rpn_loc: 0.1902    time: 5.6994  last_time: 5.4603  data_time: 0.0012  last_data_time: 0.0014   lr: 6.5275e-07  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:08:58 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:08:58 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:08:58 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:08:58 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:08:58 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:08:58 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:09:14 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.1710 s/iter. Eval: 0.0002 s/iter. Total: 1.1715 s/iter. ETA=0:01:19\n",
      "\u001B[32m[07/18 00:09:20 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0004 s/iter. Inference: 1.2048 s/iter. Eval: 0.0002 s/iter. Total: 1.2056 s/iter. ETA=0:01:15\n",
      "\u001B[32m[07/18 00:09:25 d2.evaluation.evaluator]: \u001B[0mInference done 20/79. Dataloading: 0.0004 s/iter. Inference: 1.2372 s/iter. Eval: 0.0002 s/iter. Total: 1.2380 s/iter. ETA=0:01:13\n",
      "\u001B[32m[07/18 00:09:31 d2.evaluation.evaluator]: \u001B[0mInference done 25/79. Dataloading: 0.0004 s/iter. Inference: 1.2205 s/iter. Eval: 0.0002 s/iter. Total: 1.2213 s/iter. ETA=0:01:05\n",
      "\u001B[32m[07/18 00:09:37 d2.evaluation.evaluator]: \u001B[0mInference done 30/79. Dataloading: 0.0005 s/iter. Inference: 1.2094 s/iter. Eval: 0.0002 s/iter. Total: 1.2103 s/iter. ETA=0:00:59\n",
      "\u001B[32m[07/18 00:09:43 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0005 s/iter. Inference: 1.2055 s/iter. Eval: 0.0002 s/iter. Total: 1.2063 s/iter. ETA=0:00:53\n",
      "\u001B[32m[07/18 00:09:49 d2.evaluation.evaluator]: \u001B[0mInference done 40/79. Dataloading: 0.0005 s/iter. Inference: 1.1990 s/iter. Eval: 0.0002 s/iter. Total: 1.1998 s/iter. ETA=0:00:46\n",
      "\u001B[32m[07/18 00:09:54 d2.evaluation.evaluator]: \u001B[0mInference done 45/79. Dataloading: 0.0005 s/iter. Inference: 1.1975 s/iter. Eval: 0.0002 s/iter. Total: 1.1983 s/iter. ETA=0:00:40\n",
      "\u001B[32m[07/18 00:10:00 d2.evaluation.evaluator]: \u001B[0mInference done 50/79. Dataloading: 0.0005 s/iter. Inference: 1.1939 s/iter. Eval: 0.0002 s/iter. Total: 1.1947 s/iter. ETA=0:00:34\n",
      "\u001B[32m[07/18 00:10:06 d2.evaluation.evaluator]: \u001B[0mInference done 55/79. Dataloading: 0.0005 s/iter. Inference: 1.1904 s/iter. Eval: 0.0002 s/iter. Total: 1.1912 s/iter. ETA=0:00:28\n",
      "\u001B[32m[07/18 00:10:12 d2.evaluation.evaluator]: \u001B[0mInference done 60/79. Dataloading: 0.0005 s/iter. Inference: 1.1976 s/iter. Eval: 0.0002 s/iter. Total: 1.1984 s/iter. ETA=0:00:22\n",
      "\u001B[32m[07/18 00:10:18 d2.evaluation.evaluator]: \u001B[0mInference done 65/79. Dataloading: 0.0005 s/iter. Inference: 1.1961 s/iter. Eval: 0.0002 s/iter. Total: 1.1969 s/iter. ETA=0:00:16\n",
      "\u001B[32m[07/18 00:10:24 d2.evaluation.evaluator]: \u001B[0mInference done 70/79. Dataloading: 0.0005 s/iter. Inference: 1.1928 s/iter. Eval: 0.0002 s/iter. Total: 1.1936 s/iter. ETA=0:00:10\n",
      "\u001B[32m[07/18 00:10:30 d2.evaluation.evaluator]: \u001B[0mInference done 75/79. Dataloading: 0.0005 s/iter. Inference: 1.1964 s/iter. Eval: 0.0002 s/iter. Total: 1.1972 s/iter. ETA=0:00:04\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:28.965486 (1.202236 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:28 (1.197028 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.008\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.100\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.100\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.457 | 2.074  | 0.019  |  nan  |  nan  | 0.531 |\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.012 | book       | 0.903 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:10:36 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:10:36 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:10:36 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:10:36 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:10:36 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:10:36 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:10:51 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.2393 s/iter. Eval: 0.0001 s/iter. Total: 1.2397 s/iter. ETA=0:01:24\n",
      "\u001B[32m[07/18 00:10:58 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0004 s/iter. Inference: 1.2306 s/iter. Eval: 0.0001 s/iter. Total: 1.2312 s/iter. ETA=0:01:17\n",
      "\u001B[32m[07/18 00:11:03 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0004 s/iter. Inference: 1.1982 s/iter. Eval: 0.0001 s/iter. Total: 1.1988 s/iter. ETA=0:01:09\n",
      "\u001B[32m[07/18 00:11:09 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0004 s/iter. Inference: 1.1850 s/iter. Eval: 0.0001 s/iter. Total: 1.1856 s/iter. ETA=0:01:02\n",
      "\u001B[32m[07/18 00:11:15 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0004 s/iter. Inference: 1.1906 s/iter. Eval: 0.0001 s/iter. Total: 1.1913 s/iter. ETA=0:00:57\n",
      "\u001B[32m[07/18 00:11:21 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0004 s/iter. Inference: 1.1834 s/iter. Eval: 0.0001 s/iter. Total: 1.1841 s/iter. ETA=0:00:50\n",
      "\u001B[32m[07/18 00:11:26 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 1.1778 s/iter. Eval: 0.0001 s/iter. Total: 1.1785 s/iter. ETA=0:00:44\n",
      "\u001B[32m[07/18 00:11:32 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0004 s/iter. Inference: 1.1735 s/iter. Eval: 0.0001 s/iter. Total: 1.1742 s/iter. ETA=0:00:38\n",
      "\u001B[32m[07/18 00:11:38 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0005 s/iter. Inference: 1.1756 s/iter. Eval: 0.0001 s/iter. Total: 1.1763 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 00:11:44 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0005 s/iter. Inference: 1.1730 s/iter. Eval: 0.0001 s/iter. Total: 1.1737 s/iter. ETA=0:00:26\n",
      "\u001B[32m[07/18 00:11:50 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0005 s/iter. Inference: 1.1770 s/iter. Eval: 0.0001 s/iter. Total: 1.1778 s/iter. ETA=0:00:21\n",
      "\u001B[32m[07/18 00:11:56 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0005 s/iter. Inference: 1.1811 s/iter. Eval: 0.0001 s/iter. Total: 1.1818 s/iter. ETA=0:00:15\n",
      "\u001B[32m[07/18 00:12:02 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0005 s/iter. Inference: 1.1786 s/iter. Eval: 0.0001 s/iter. Total: 1.1794 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 00:12:08 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0005 s/iter. Inference: 1.1755 s/iter. Eval: 0.0001 s/iter. Total: 1.1762 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:27.470410 (1.182033 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:27 (1.176691 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.008\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.100\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.100\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.457 | 2.074  | 0.019  |  nan  |  nan  | 0.531 |\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:12:12 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.012 | book       | 0.903 |\n",
      "\u001B[32m[07/18 00:12:12 d2.utils.events]: \u001B[0m eta: 0:59:15  iter: 199  total_loss: 2.071  loss_cls: 0.972  loss_box_reg: 0.8906  loss_rpn_cls: 0.02046  loss_rpn_loc: 0.1825    time: 6.1470  last_time: 102.5798  data_time: 0.0012  last_data_time: 0.0011   lr: 6.9775e-07  \n",
      "\u001B[32m[07/18 00:13:59 d2.utils.events]: \u001B[0m eta: 0:58:12  iter: 219  total_loss: 2.085  loss_cls: 0.9578  loss_box_reg: 0.9117  loss_rpn_cls: 0.02018  loss_rpn_loc: 0.192    time: 6.0748  last_time: 5.4740  data_time: 0.0012  last_data_time: 0.0010   lr: 7.4275e-07  \n",
      "\u001B[32m[07/18 00:16:00 d2.utils.events]: \u001B[0m eta: 0:57:13  iter: 239  total_loss: 2.057  loss_cls: 0.9513  loss_box_reg: 0.8856  loss_rpn_cls: 0.02164  loss_rpn_loc: 0.1793    time: 6.0737  last_time: 6.8671  data_time: 0.0012  last_data_time: 0.0011   lr: 7.8775e-07  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:17:06 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:17:06 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:17:06 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:17:06 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:17:06 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:17:06 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:17:23 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.3774 s/iter. Eval: 0.0002 s/iter. Total: 1.3780 s/iter. ETA=0:01:33\n",
      "\u001B[32m[07/18 00:17:28 d2.evaluation.evaluator]: \u001B[0mInference done 14/79. Dataloading: 0.0004 s/iter. Inference: 1.5153 s/iter. Eval: 0.0002 s/iter. Total: 1.5161 s/iter. ETA=0:01:38\n",
      "\u001B[32m[07/18 00:17:34 d2.evaluation.evaluator]: \u001B[0mInference done 18/79. Dataloading: 0.0005 s/iter. Inference: 1.4656 s/iter. Eval: 0.0002 s/iter. Total: 1.4664 s/iter. ETA=0:01:29\n",
      "\u001B[32m[07/18 00:17:40 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0005 s/iter. Inference: 1.5846 s/iter. Eval: 0.0003 s/iter. Total: 1.5856 s/iter. ETA=0:01:31\n",
      "\u001B[32m[07/18 00:17:46 d2.evaluation.evaluator]: \u001B[0mInference done 24/79. Dataloading: 0.0006 s/iter. Inference: 1.6402 s/iter. Eval: 0.0003 s/iter. Total: 1.6412 s/iter. ETA=0:01:30\n",
      "\u001B[32m[07/18 00:17:52 d2.evaluation.evaluator]: \u001B[0mInference done 28/79. Dataloading: 0.0006 s/iter. Inference: 1.6263 s/iter. Eval: 0.0003 s/iter. Total: 1.6274 s/iter. ETA=0:01:22\n",
      "\u001B[32m[07/18 00:17:58 d2.evaluation.evaluator]: \u001B[0mInference done 32/79. Dataloading: 0.0006 s/iter. Inference: 1.5965 s/iter. Eval: 0.0003 s/iter. Total: 1.5975 s/iter. ETA=0:01:15\n",
      "\u001B[32m[07/18 00:18:04 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0006 s/iter. Inference: 1.5790 s/iter. Eval: 0.0003 s/iter. Total: 1.5800 s/iter. ETA=0:01:07\n",
      "\u001B[32m[07/18 00:18:09 d2.evaluation.evaluator]: \u001B[0mInference done 40/79. Dataloading: 0.0006 s/iter. Inference: 1.5584 s/iter. Eval: 0.0003 s/iter. Total: 1.5594 s/iter. ETA=0:01:00\n",
      "\u001B[32m[07/18 00:18:15 d2.evaluation.evaluator]: \u001B[0mInference done 44/79. Dataloading: 0.0005 s/iter. Inference: 1.5402 s/iter. Eval: 0.0002 s/iter. Total: 1.5412 s/iter. ETA=0:00:53\n",
      "\u001B[32m[07/18 00:18:20 d2.evaluation.evaluator]: \u001B[0mInference done 48/79. Dataloading: 0.0005 s/iter. Inference: 1.5194 s/iter. Eval: 0.0002 s/iter. Total: 1.5204 s/iter. ETA=0:00:47\n",
      "\u001B[32m[07/18 00:18:25 d2.evaluation.evaluator]: \u001B[0mInference done 52/79. Dataloading: 0.0005 s/iter. Inference: 1.4980 s/iter. Eval: 0.0002 s/iter. Total: 1.4989 s/iter. ETA=0:00:40\n",
      "\u001B[32m[07/18 00:18:30 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0005 s/iter. Inference: 1.4788 s/iter. Eval: 0.0002 s/iter. Total: 1.4797 s/iter. ETA=0:00:34\n",
      "\u001B[32m[07/18 00:18:35 d2.evaluation.evaluator]: \u001B[0mInference done 60/79. Dataloading: 0.0005 s/iter. Inference: 1.4636 s/iter. Eval: 0.0002 s/iter. Total: 1.4645 s/iter. ETA=0:00:27\n",
      "\u001B[32m[07/18 00:18:41 d2.evaluation.evaluator]: \u001B[0mInference done 65/79. Dataloading: 0.0005 s/iter. Inference: 1.4392 s/iter. Eval: 0.0002 s/iter. Total: 1.4401 s/iter. ETA=0:00:20\n",
      "\u001B[32m[07/18 00:18:47 d2.evaluation.evaluator]: \u001B[0mInference done 70/79. Dataloading: 0.0005 s/iter. Inference: 1.4220 s/iter. Eval: 0.0002 s/iter. Total: 1.4229 s/iter. ETA=0:00:12\n",
      "\u001B[32m[07/18 00:18:53 d2.evaluation.evaluator]: \u001B[0mInference done 75/79. Dataloading: 0.0005 s/iter. Inference: 1.4083 s/iter. Eval: 0.0002 s/iter. Total: 1.4092 s/iter. ETA=0:00:05\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:43.846851 (1.403336 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:43 (1.397478 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.007\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.080\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.080\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.586 | 2.598  | 0.022  |  nan  |  nan  | 0.674 |\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.009 | book       | 1.163 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:18:59 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:18:59 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:18:59 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:18:59 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:18:59 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:18:59 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:19:15 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.3158 s/iter. Eval: 0.0002 s/iter. Total: 1.3163 s/iter. ETA=0:01:29\n",
      "\u001B[32m[07/18 00:19:21 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0004 s/iter. Inference: 1.3004 s/iter. Eval: 0.0002 s/iter. Total: 1.3011 s/iter. ETA=0:01:23\n",
      "\u001B[32m[07/18 00:19:26 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0004 s/iter. Inference: 1.2866 s/iter. Eval: 0.0002 s/iter. Total: 1.2873 s/iter. ETA=0:01:17\n",
      "\u001B[32m[07/18 00:19:32 d2.evaluation.evaluator]: \u001B[0mInference done 24/79. Dataloading: 0.0004 s/iter. Inference: 1.2686 s/iter. Eval: 0.0002 s/iter. Total: 1.2692 s/iter. ETA=0:01:09\n",
      "\u001B[32m[07/18 00:19:37 d2.evaluation.evaluator]: \u001B[0mInference done 28/79. Dataloading: 0.0004 s/iter. Inference: 1.2666 s/iter. Eval: 0.0002 s/iter. Total: 1.2673 s/iter. ETA=0:01:04\n",
      "\u001B[32m[07/18 00:19:42 d2.evaluation.evaluator]: \u001B[0mInference done 32/79. Dataloading: 0.0004 s/iter. Inference: 1.2770 s/iter. Eval: 0.0002 s/iter. Total: 1.2777 s/iter. ETA=0:01:00\n",
      "\u001B[32m[07/18 00:19:47 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0004 s/iter. Inference: 1.2746 s/iter. Eval: 0.0002 s/iter. Total: 1.2753 s/iter. ETA=0:00:54\n",
      "\u001B[32m[07/18 00:19:53 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 1.2627 s/iter. Eval: 0.0002 s/iter. Total: 1.2634 s/iter. ETA=0:00:48\n",
      "\u001B[32m[07/18 00:19:59 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0004 s/iter. Inference: 1.2633 s/iter. Eval: 0.0002 s/iter. Total: 1.2641 s/iter. ETA=0:00:41\n",
      "\u001B[32m[07/18 00:20:05 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0004 s/iter. Inference: 1.2576 s/iter. Eval: 0.0002 s/iter. Total: 1.2584 s/iter. ETA=0:00:35\n",
      "\u001B[32m[07/18 00:20:11 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0004 s/iter. Inference: 1.2489 s/iter. Eval: 0.0002 s/iter. Total: 1.2496 s/iter. ETA=0:00:28\n",
      "\u001B[32m[07/18 00:20:17 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0004 s/iter. Inference: 1.2479 s/iter. Eval: 0.0002 s/iter. Total: 1.2487 s/iter. ETA=0:00:22\n",
      "\u001B[32m[07/18 00:20:24 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0004 s/iter. Inference: 1.2445 s/iter. Eval: 0.0002 s/iter. Total: 1.2453 s/iter. ETA=0:00:16\n",
      "\u001B[32m[07/18 00:20:30 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0004 s/iter. Inference: 1.2458 s/iter. Eval: 0.0002 s/iter. Total: 1.2465 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 00:20:36 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0004 s/iter. Inference: 1.2430 s/iter. Eval: 0.0002 s/iter. Total: 1.2438 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:32.391081 (1.248528 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:31 (1.242984 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.007\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.080\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.080\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.586 | 2.598  | 0.022  |  nan  |  nan  | 0.674 |\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:20:40 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.009 | book       | 1.163 |\n",
      "\u001B[32m[07/18 00:21:32 d2.utils.events]: \u001B[0m eta: 0:55:59  iter: 259  total_loss: 2.064  loss_cls: 0.944  loss_box_reg: 0.9024  loss_rpn_cls: 0.02243  loss_rpn_loc: 0.1875    time: 6.4959  last_time: 5.3952  data_time: 0.0012  last_data_time: 0.0014   lr: 8.3275e-07  \n",
      "\u001B[32m[07/18 00:23:19 d2.utils.events]: \u001B[0m eta: 0:55:02  iter: 279  total_loss: 2.026  loss_cls: 0.9349  loss_box_reg: 0.897  loss_rpn_cls: 0.0184  loss_rpn_loc: 0.1762    time: 6.4154  last_time: 5.5629  data_time: 0.0014  last_data_time: 0.0008   lr: 8.7775e-07  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:25:08 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:25:08 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:25:08 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:25:08 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:25:08 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:25:08 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:25:25 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.2411 s/iter. Eval: 0.0002 s/iter. Total: 1.2416 s/iter. ETA=0:01:24\n",
      "\u001B[32m[07/18 00:25:30 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0004 s/iter. Inference: 1.2663 s/iter. Eval: 0.0002 s/iter. Total: 1.2670 s/iter. ETA=0:01:21\n",
      "\u001B[32m[07/18 00:25:35 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0005 s/iter. Inference: 1.2758 s/iter. Eval: 0.0002 s/iter. Total: 1.2766 s/iter. ETA=0:01:16\n",
      "\u001B[32m[07/18 00:25:41 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0005 s/iter. Inference: 1.3124 s/iter. Eval: 0.0002 s/iter. Total: 1.3133 s/iter. ETA=0:01:13\n",
      "\u001B[32m[07/18 00:25:46 d2.evaluation.evaluator]: \u001B[0mInference done 27/79. Dataloading: 0.0006 s/iter. Inference: 1.3270 s/iter. Eval: 0.0002 s/iter. Total: 1.3279 s/iter. ETA=0:01:09\n",
      "\u001B[32m[07/18 00:25:52 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.3356 s/iter. Eval: 0.0002 s/iter. Total: 1.3365 s/iter. ETA=0:01:04\n",
      "\u001B[32m[07/18 00:25:57 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0005 s/iter. Inference: 1.3312 s/iter. Eval: 0.0002 s/iter. Total: 1.3321 s/iter. ETA=0:00:58\n",
      "\u001B[32m[07/18 00:26:03 d2.evaluation.evaluator]: \u001B[0mInference done 39/79. Dataloading: 0.0006 s/iter. Inference: 1.3338 s/iter. Eval: 0.0002 s/iter. Total: 1.3347 s/iter. ETA=0:00:53\n",
      "\u001B[32m[07/18 00:26:08 d2.evaluation.evaluator]: \u001B[0mInference done 43/79. Dataloading: 0.0005 s/iter. Inference: 1.3251 s/iter. Eval: 0.0002 s/iter. Total: 1.3260 s/iter. ETA=0:00:47\n",
      "\u001B[32m[07/18 00:26:13 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0006 s/iter. Inference: 1.3224 s/iter. Eval: 0.0002 s/iter. Total: 1.3234 s/iter. ETA=0:00:42\n",
      "\u001B[32m[07/18 00:26:18 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0006 s/iter. Inference: 1.3188 s/iter. Eval: 0.0002 s/iter. Total: 1.3197 s/iter. ETA=0:00:36\n",
      "\u001B[32m[07/18 00:26:24 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0006 s/iter. Inference: 1.3124 s/iter. Eval: 0.0002 s/iter. Total: 1.3134 s/iter. ETA=0:00:30\n",
      "\u001B[32m[07/18 00:26:30 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0006 s/iter. Inference: 1.3029 s/iter. Eval: 0.0002 s/iter. Total: 1.3039 s/iter. ETA=0:00:23\n",
      "\u001B[32m[07/18 00:26:37 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0006 s/iter. Inference: 1.3017 s/iter. Eval: 0.0002 s/iter. Total: 1.3027 s/iter. ETA=0:00:16\n",
      "\u001B[32m[07/18 00:26:43 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0006 s/iter. Inference: 1.2939 s/iter. Eval: 0.0002 s/iter. Total: 1.2948 s/iter. ETA=0:00:10\n",
      "\u001B[32m[07/18 00:26:49 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0005 s/iter. Inference: 1.2860 s/iter. Eval: 0.0002 s/iter. Total: 1.2869 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:35.495194 (1.290476 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:35 (1.284815 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.008\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.024\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.090\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.090\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.716 | 3.131  | 0.031  |  nan  |  nan  | 0.815 |\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.010 | book       | 1.423 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:26:53 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:26:53 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:26:53 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:26:53 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:26:53 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:26:53 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:27:11 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.2675 s/iter. Eval: 0.0002 s/iter. Total: 1.2680 s/iter. ETA=0:01:26\n",
      "\u001B[32m[07/18 00:27:16 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0005 s/iter. Inference: 1.2679 s/iter. Eval: 0.0002 s/iter. Total: 1.2687 s/iter. ETA=0:01:21\n",
      "\u001B[32m[07/18 00:27:21 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0005 s/iter. Inference: 1.2663 s/iter. Eval: 0.0002 s/iter. Total: 1.2671 s/iter. ETA=0:01:16\n",
      "\u001B[32m[07/18 00:27:26 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0005 s/iter. Inference: 1.2702 s/iter. Eval: 0.0002 s/iter. Total: 1.2711 s/iter. ETA=0:01:11\n",
      "\u001B[32m[07/18 00:27:31 d2.evaluation.evaluator]: \u001B[0mInference done 27/79. Dataloading: 0.0005 s/iter. Inference: 1.2671 s/iter. Eval: 0.0002 s/iter. Total: 1.2680 s/iter. ETA=0:01:05\n",
      "\u001B[32m[07/18 00:27:37 d2.evaluation.evaluator]: \u001B[0mInference done 32/79. Dataloading: 0.0005 s/iter. Inference: 1.2584 s/iter. Eval: 0.0002 s/iter. Total: 1.2592 s/iter. ETA=0:00:59\n",
      "\u001B[32m[07/18 00:27:42 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0006 s/iter. Inference: 1.2598 s/iter. Eval: 0.0002 s/iter. Total: 1.2608 s/iter. ETA=0:00:54\n",
      "\u001B[32m[07/18 00:27:48 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0005 s/iter. Inference: 1.2578 s/iter. Eval: 0.0002 s/iter. Total: 1.2587 s/iter. ETA=0:00:47\n",
      "\u001B[32m[07/18 00:27:55 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0005 s/iter. Inference: 1.2613 s/iter. Eval: 0.0002 s/iter. Total: 1.2622 s/iter. ETA=0:00:41\n",
      "\u001B[32m[07/18 00:28:00 d2.evaluation.evaluator]: \u001B[0mInference done 50/79. Dataloading: 0.0005 s/iter. Inference: 1.2652 s/iter. Eval: 0.0002 s/iter. Total: 1.2661 s/iter. ETA=0:00:36\n",
      "\u001B[32m[07/18 00:28:05 d2.evaluation.evaluator]: \u001B[0mInference done 54/79. Dataloading: 0.0005 s/iter. Inference: 1.2722 s/iter. Eval: 0.0002 s/iter. Total: 1.2730 s/iter. ETA=0:00:31\n",
      "\u001B[32m[07/18 00:28:11 d2.evaluation.evaluator]: \u001B[0mInference done 58/79. Dataloading: 0.0005 s/iter. Inference: 1.2728 s/iter. Eval: 0.0002 s/iter. Total: 1.2736 s/iter. ETA=0:00:26\n",
      "\u001B[32m[07/18 00:28:16 d2.evaluation.evaluator]: \u001B[0mInference done 63/79. Dataloading: 0.0005 s/iter. Inference: 1.2651 s/iter. Eval: 0.0002 s/iter. Total: 1.2660 s/iter. ETA=0:00:20\n",
      "\u001B[32m[07/18 00:28:22 d2.evaluation.evaluator]: \u001B[0mInference done 67/79. Dataloading: 0.0005 s/iter. Inference: 1.2666 s/iter. Eval: 0.0002 s/iter. Total: 1.2675 s/iter. ETA=0:00:15\n",
      "\u001B[32m[07/18 00:28:27 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0005 s/iter. Inference: 1.2669 s/iter. Eval: 0.0002 s/iter. Total: 1.2677 s/iter. ETA=0:00:10\n",
      "\u001B[32m[07/18 00:28:32 d2.evaluation.evaluator]: \u001B[0mInference done 75/79. Dataloading: 0.0005 s/iter. Inference: 1.2675 s/iter. Eval: 0.0002 s/iter. Total: 1.2684 s/iter. ETA=0:00:05\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.evaluator]: \u001B[0mInference done 79/79. Dataloading: 0.0005 s/iter. Inference: 1.2668 s/iter. Eval: 0.0002 s/iter. Total: 1.2677 s/iter. ETA=0:00:00\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:34.256035 (1.273730 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:33 (1.266830 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.008\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.024\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.090\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.090\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.716 | 3.131  | 0.031  |  nan  |  nan  | 0.815 |\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:28:37 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.010 | book       | 1.423 |\n",
      "\u001B[32m[07/18 00:28:37 d2.utils.events]: \u001B[0m eta: 0:54:20  iter: 299  total_loss: 2.02  loss_cls: 0.9291  loss_box_reg: 0.892  loss_rpn_cls: 0.01751  loss_rpn_loc: 0.1891    time: 6.7008  last_time: 110.2754  data_time: 0.0011  last_data_time: 0.0008   lr: 9.2275e-07  \n",
      "\u001B[32m[07/18 00:30:28 d2.utils.events]: \u001B[0m eta: 0:54:48  iter: 319  total_loss: 2.026  loss_cls: 0.9212  loss_box_reg: 0.8864  loss_rpn_cls: 0.02105  loss_rpn_loc: 0.1896    time: 6.6284  last_time: 5.5389  data_time: 0.0011  last_data_time: 0.0009   lr: 9.6775e-07  \n",
      "\u001B[32m[07/18 00:32:15 d2.utils.events]: \u001B[0m eta: 0:54:44  iter: 339  total_loss: 2.008  loss_cls: 0.9081  loss_box_reg: 0.8865  loss_rpn_cls: 0.02291  loss_rpn_loc: 0.1764    time: 6.5528  last_time: 5.7839  data_time: 0.0014  last_data_time: 0.0018   lr: 1.0128e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:33:10 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:33:10 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:33:10 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:33:10 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:33:10 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:33:10 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:33:26 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.2663 s/iter. Eval: 0.0002 s/iter. Total: 1.2669 s/iter. ETA=0:01:26\n",
      "\u001B[32m[07/18 00:33:32 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0004 s/iter. Inference: 1.2548 s/iter. Eval: 0.0002 s/iter. Total: 1.2555 s/iter. ETA=0:01:19\n",
      "\u001B[32m[07/18 00:33:38 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0005 s/iter. Inference: 1.2378 s/iter. Eval: 0.0002 s/iter. Total: 1.2386 s/iter. ETA=0:01:11\n",
      "\u001B[32m[07/18 00:33:44 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0005 s/iter. Inference: 1.2362 s/iter. Eval: 0.0002 s/iter. Total: 1.2370 s/iter. ETA=0:01:05\n",
      "\u001B[32m[07/18 00:33:51 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.2333 s/iter. Eval: 0.0002 s/iter. Total: 1.2341 s/iter. ETA=0:00:59\n",
      "\u001B[32m[07/18 00:33:56 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0005 s/iter. Inference: 1.2272 s/iter. Eval: 0.0002 s/iter. Total: 1.2280 s/iter. ETA=0:00:52\n",
      "\u001B[32m[07/18 00:34:02 d2.evaluation.evaluator]: \u001B[0mInference done 40/79. Dataloading: 0.0005 s/iter. Inference: 1.2315 s/iter. Eval: 0.0002 s/iter. Total: 1.2323 s/iter. ETA=0:00:48\n",
      "\u001B[32m[07/18 00:34:08 d2.evaluation.evaluator]: \u001B[0mInference done 45/79. Dataloading: 0.0005 s/iter. Inference: 1.2287 s/iter. Eval: 0.0002 s/iter. Total: 1.2296 s/iter. ETA=0:00:41\n",
      "\u001B[32m[07/18 00:34:14 d2.evaluation.evaluator]: \u001B[0mInference done 50/79. Dataloading: 0.0005 s/iter. Inference: 1.2244 s/iter. Eval: 0.0002 s/iter. Total: 1.2253 s/iter. ETA=0:00:35\n",
      "\u001B[32m[07/18 00:34:20 d2.evaluation.evaluator]: \u001B[0mInference done 55/79. Dataloading: 0.0005 s/iter. Inference: 1.2279 s/iter. Eval: 0.0002 s/iter. Total: 1.2288 s/iter. ETA=0:00:29\n",
      "\u001B[32m[07/18 00:34:25 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0005 s/iter. Inference: 1.2304 s/iter. Eval: 0.0002 s/iter. Total: 1.2312 s/iter. ETA=0:00:24\n",
      "\u001B[32m[07/18 00:34:31 d2.evaluation.evaluator]: \u001B[0mInference done 64/79. Dataloading: 0.0005 s/iter. Inference: 1.2272 s/iter. Eval: 0.0002 s/iter. Total: 1.2280 s/iter. ETA=0:00:18\n",
      "\u001B[32m[07/18 00:34:36 d2.evaluation.evaluator]: \u001B[0mInference done 68/79. Dataloading: 0.0005 s/iter. Inference: 1.2296 s/iter. Eval: 0.0002 s/iter. Total: 1.2304 s/iter. ETA=0:00:13\n",
      "\u001B[32m[07/18 00:34:41 d2.evaluation.evaluator]: \u001B[0mInference done 72/79. Dataloading: 0.0005 s/iter. Inference: 1.2313 s/iter. Eval: 0.0002 s/iter. Total: 1.2321 s/iter. ETA=0:00:08\n",
      "\u001B[32m[07/18 00:34:47 d2.evaluation.evaluator]: \u001B[0mInference done 77/79. Dataloading: 0.0005 s/iter. Inference: 1.2262 s/iter. Eval: 0.0002 s/iter. Total: 1.2270 s/iter. ETA=0:00:02\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:31.262311 (1.233274 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:30 (1.226560 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.009\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.025\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.098\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.098\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.820 | 3.527  | 0.031  |  nan  |  nan  | 0.934 |\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.012 | book       | 1.629 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:34:50 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:34:50 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:34:50 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:34:50 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:34:50 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:34:50 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:35:06 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.2373 s/iter. Eval: 0.0002 s/iter. Total: 1.2378 s/iter. ETA=0:01:24\n",
      "\u001B[32m[07/18 00:35:12 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0005 s/iter. Inference: 1.2424 s/iter. Eval: 0.0002 s/iter. Total: 1.2432 s/iter. ETA=0:01:18\n",
      "\u001B[32m[07/18 00:35:18 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0005 s/iter. Inference: 1.2209 s/iter. Eval: 0.0002 s/iter. Total: 1.2217 s/iter. ETA=0:01:10\n",
      "\u001B[32m[07/18 00:35:24 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0005 s/iter. Inference: 1.2159 s/iter. Eval: 0.0002 s/iter. Total: 1.2167 s/iter. ETA=0:01:04\n",
      "\u001B[32m[07/18 00:35:31 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.2215 s/iter. Eval: 0.0002 s/iter. Total: 1.2223 s/iter. ETA=0:00:58\n",
      "\u001B[32m[07/18 00:35:37 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0005 s/iter. Inference: 1.2166 s/iter. Eval: 0.0002 s/iter. Total: 1.2174 s/iter. ETA=0:00:52\n",
      "\u001B[32m[07/18 00:35:43 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0005 s/iter. Inference: 1.2171 s/iter. Eval: 0.0002 s/iter. Total: 1.2180 s/iter. ETA=0:00:46\n",
      "\u001B[32m[07/18 00:35:49 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0005 s/iter. Inference: 1.2173 s/iter. Eval: 0.0002 s/iter. Total: 1.2181 s/iter. ETA=0:00:40\n",
      "\u001B[32m[07/18 00:35:55 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0005 s/iter. Inference: 1.2152 s/iter. Eval: 0.0002 s/iter. Total: 1.2160 s/iter. ETA=0:00:34\n",
      "\u001B[32m[07/18 00:36:01 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0005 s/iter. Inference: 1.2150 s/iter. Eval: 0.0002 s/iter. Total: 1.2159 s/iter. ETA=0:00:27\n",
      "\u001B[32m[07/18 00:36:07 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0005 s/iter. Inference: 1.2168 s/iter. Eval: 0.0002 s/iter. Total: 1.2177 s/iter. ETA=0:00:21\n",
      "\u001B[32m[07/18 00:36:13 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0006 s/iter. Inference: 1.2147 s/iter. Eval: 0.0002 s/iter. Total: 1.2156 s/iter. ETA=0:00:15\n",
      "\u001B[32m[07/18 00:36:18 d2.evaluation.evaluator]: \u001B[0mInference done 70/79. Dataloading: 0.0006 s/iter. Inference: 1.2184 s/iter. Eval: 0.0002 s/iter. Total: 1.2193 s/iter. ETA=0:00:10\n",
      "\u001B[32m[07/18 00:36:24 d2.evaluation.evaluator]: \u001B[0mInference done 75/79. Dataloading: 0.0006 s/iter. Inference: 1.2165 s/iter. Eval: 0.0002 s/iter. Total: 1.2173 s/iter. ETA=0:00:04\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:30.379496 (1.221345 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:29 (1.214285 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.13s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.009\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.025\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.098\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.098\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.820 | 3.527  | 0.031  |  nan  |  nan  | 0.934 |\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:36:29 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.012 | book       | 1.629 |\n",
      "\u001B[32m[07/18 00:37:23 d2.utils.events]: \u001B[0m eta: 0:53:52  iter: 359  total_loss: 1.953  loss_cls: 0.8997  loss_box_reg: 0.8633  loss_rpn_cls: 0.01742  loss_rpn_loc: 0.1736    time: 6.7669  last_time: 5.4864  data_time: 0.0013  last_data_time: 0.0010   lr: 1.0578e-06  \n",
      "\u001B[32m[07/18 00:39:11 d2.utils.events]: \u001B[0m eta: 0:52:41  iter: 379  total_loss: 1.968  loss_cls: 0.8892  loss_box_reg: 0.8747  loss_rpn_cls: 0.02173  loss_rpn_loc: 0.1733    time: 6.6956  last_time: 4.3078  data_time: 0.0013  last_data_time: 0.0016   lr: 1.1028e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:41:05 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:41:05 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:41:05 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:41:05 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:41:05 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:41:05 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:41:23 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.4007 s/iter. Eval: 0.0002 s/iter. Total: 1.4013 s/iter. ETA=0:01:35\n",
      "\u001B[32m[07/18 00:41:29 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0004 s/iter. Inference: 1.4142 s/iter. Eval: 0.0002 s/iter. Total: 1.4149 s/iter. ETA=0:01:30\n",
      "\u001B[32m[07/18 00:41:35 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0004 s/iter. Inference: 1.4042 s/iter. Eval: 0.0002 s/iter. Total: 1.4049 s/iter. ETA=0:01:24\n",
      "\u001B[32m[07/18 00:41:40 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0004 s/iter. Inference: 1.3893 s/iter. Eval: 0.0002 s/iter. Total: 1.3900 s/iter. ETA=0:01:17\n",
      "\u001B[32m[07/18 00:41:46 d2.evaluation.evaluator]: \u001B[0mInference done 27/79. Dataloading: 0.0005 s/iter. Inference: 1.4130 s/iter. Eval: 0.0002 s/iter. Total: 1.4139 s/iter. ETA=0:01:13\n",
      "\u001B[32m[07/18 00:41:52 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.4367 s/iter. Eval: 0.0002 s/iter. Total: 1.4376 s/iter. ETA=0:01:09\n",
      "\u001B[32m[07/18 00:41:58 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0005 s/iter. Inference: 1.4246 s/iter. Eval: 0.0002 s/iter. Total: 1.4256 s/iter. ETA=0:01:02\n",
      "\u001B[32m[07/18 00:42:03 d2.evaluation.evaluator]: \u001B[0mInference done 39/79. Dataloading: 0.0005 s/iter. Inference: 1.4199 s/iter. Eval: 0.0002 s/iter. Total: 1.4208 s/iter. ETA=0:00:56\n",
      "\u001B[32m[07/18 00:42:09 d2.evaluation.evaluator]: \u001B[0mInference done 43/79. Dataloading: 0.0005 s/iter. Inference: 1.4252 s/iter. Eval: 0.0002 s/iter. Total: 1.4261 s/iter. ETA=0:00:51\n",
      "\u001B[32m[07/18 00:42:15 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0005 s/iter. Inference: 1.4304 s/iter. Eval: 0.0002 s/iter. Total: 1.4313 s/iter. ETA=0:00:45\n",
      "\u001B[32m[07/18 00:42:21 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0005 s/iter. Inference: 1.4277 s/iter. Eval: 0.0002 s/iter. Total: 1.4286 s/iter. ETA=0:00:40\n",
      "\u001B[32m[07/18 00:42:26 d2.evaluation.evaluator]: \u001B[0mInference done 55/79. Dataloading: 0.0005 s/iter. Inference: 1.4270 s/iter. Eval: 0.0002 s/iter. Total: 1.4280 s/iter. ETA=0:00:34\n",
      "\u001B[32m[07/18 00:42:32 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0005 s/iter. Inference: 1.4262 s/iter. Eval: 0.0002 s/iter. Total: 1.4272 s/iter. ETA=0:00:28\n",
      "\u001B[32m[07/18 00:42:37 d2.evaluation.evaluator]: \u001B[0mInference done 63/79. Dataloading: 0.0005 s/iter. Inference: 1.4236 s/iter. Eval: 0.0002 s/iter. Total: 1.4245 s/iter. ETA=0:00:22\n",
      "\u001B[32m[07/18 00:42:44 d2.evaluation.evaluator]: \u001B[0mInference done 67/79. Dataloading: 0.0005 s/iter. Inference: 1.4342 s/iter. Eval: 0.0002 s/iter. Total: 1.4351 s/iter. ETA=0:00:17\n",
      "\u001B[32m[07/18 00:42:50 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0005 s/iter. Inference: 1.4387 s/iter. Eval: 0.0002 s/iter. Total: 1.4396 s/iter. ETA=0:00:11\n",
      "\u001B[32m[07/18 00:42:56 d2.evaluation.evaluator]: \u001B[0mInference done 75/79. Dataloading: 0.0005 s/iter. Inference: 1.4390 s/iter. Eval: 0.0002 s/iter. Total: 1.4399 s/iter. ETA=0:00:05\n",
      "\u001B[32m[07/18 00:43:01 d2.evaluation.evaluator]: \u001B[0mInference done 79/79. Dataloading: 0.0005 s/iter. Inference: 1.4391 s/iter. Eval: 0.0002 s/iter. Total: 1.4400 s/iter. ETA=0:00:00\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:46.924614 (1.444927 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:46 (1.439131 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.039\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.011\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.026\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.107\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.107\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.939 | 3.860  | 0.042  |  nan  |  nan  | 1.069 |\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.014 | book       | 1.865 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:43:02 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:43:02 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:43:02 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:43:02 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:43:02 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:43:02 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:43:20 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.3523 s/iter. Eval: 0.0002 s/iter. Total: 1.3528 s/iter. ETA=0:01:31\n",
      "\u001B[32m[07/18 00:43:25 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0005 s/iter. Inference: 1.3258 s/iter. Eval: 0.0002 s/iter. Total: 1.3266 s/iter. ETA=0:01:24\n",
      "\u001B[32m[07/18 00:43:30 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0005 s/iter. Inference: 1.3175 s/iter. Eval: 0.0002 s/iter. Total: 1.3183 s/iter. ETA=0:01:19\n",
      "\u001B[32m[07/18 00:43:35 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0005 s/iter. Inference: 1.3189 s/iter. Eval: 0.0002 s/iter. Total: 1.3197 s/iter. ETA=0:01:13\n",
      "\u001B[32m[07/18 00:43:41 d2.evaluation.evaluator]: \u001B[0mInference done 27/79. Dataloading: 0.0006 s/iter. Inference: 1.3208 s/iter. Eval: 0.0002 s/iter. Total: 1.3217 s/iter. ETA=0:01:08\n",
      "\u001B[32m[07/18 00:43:46 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.3221 s/iter. Eval: 0.0002 s/iter. Total: 1.3230 s/iter. ETA=0:01:03\n",
      "\u001B[32m[07/18 00:43:52 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0005 s/iter. Inference: 1.3312 s/iter. Eval: 0.0002 s/iter. Total: 1.3320 s/iter. ETA=0:00:58\n",
      "\u001B[32m[07/18 00:43:57 d2.evaluation.evaluator]: \u001B[0mInference done 39/79. Dataloading: 0.0005 s/iter. Inference: 1.3355 s/iter. Eval: 0.0002 s/iter. Total: 1.3363 s/iter. ETA=0:00:53\n",
      "\u001B[32m[07/18 00:44:03 d2.evaluation.evaluator]: \u001B[0mInference done 43/79. Dataloading: 0.0005 s/iter. Inference: 1.3425 s/iter. Eval: 0.0002 s/iter. Total: 1.3434 s/iter. ETA=0:00:48\n",
      "\u001B[32m[07/18 00:44:08 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0005 s/iter. Inference: 1.3482 s/iter. Eval: 0.0002 s/iter. Total: 1.3491 s/iter. ETA=0:00:43\n",
      "\u001B[32m[07/18 00:44:14 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0005 s/iter. Inference: 1.3555 s/iter. Eval: 0.0002 s/iter. Total: 1.3564 s/iter. ETA=0:00:37\n",
      "\u001B[32m[07/18 00:44:19 d2.evaluation.evaluator]: \u001B[0mInference done 55/79. Dataloading: 0.0005 s/iter. Inference: 1.3548 s/iter. Eval: 0.0002 s/iter. Total: 1.3556 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 00:44:25 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0005 s/iter. Inference: 1.3535 s/iter. Eval: 0.0002 s/iter. Total: 1.3544 s/iter. ETA=0:00:27\n",
      "\u001B[32m[07/18 00:44:30 d2.evaluation.evaluator]: \u001B[0mInference done 63/79. Dataloading: 0.0005 s/iter. Inference: 1.3522 s/iter. Eval: 0.0002 s/iter. Total: 1.3531 s/iter. ETA=0:00:21\n",
      "\u001B[32m[07/18 00:44:36 d2.evaluation.evaluator]: \u001B[0mInference done 67/79. Dataloading: 0.0005 s/iter. Inference: 1.3554 s/iter. Eval: 0.0002 s/iter. Total: 1.3563 s/iter. ETA=0:00:16\n",
      "\u001B[32m[07/18 00:44:42 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0005 s/iter. Inference: 1.3617 s/iter. Eval: 0.0002 s/iter. Total: 1.3625 s/iter. ETA=0:00:10\n",
      "\u001B[32m[07/18 00:44:47 d2.evaluation.evaluator]: \u001B[0mInference done 75/79. Dataloading: 0.0005 s/iter. Inference: 1.3657 s/iter. Eval: 0.0002 s/iter. Total: 1.3665 s/iter. ETA=0:00:05\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.evaluator]: \u001B[0mInference done 79/79. Dataloading: 0.0005 s/iter. Inference: 1.3642 s/iter. Eval: 0.0002 s/iter. Total: 1.3650 s/iter. ETA=0:00:00\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:41.349787 (1.369592 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:40 (1.364154 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.039\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.011\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.026\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.107\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.107\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.939 | 3.860  | 0.042  |  nan  |  nan  | 1.069 |\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:44:53 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.014 | book       | 1.865 |\n",
      "\u001B[32m[07/18 00:44:53 d2.utils.events]: \u001B[0m eta: 0:51:15  iter: 399  total_loss: 1.947  loss_cls: 0.8771  loss_box_reg: 0.8541  loss_rpn_cls: 0.02774  loss_rpn_loc: 0.1759    time: 6.9385  last_time: 121.5447  data_time: 0.0010  last_data_time: 0.0009   lr: 1.1478e-06  \n",
      "\u001B[32m[07/18 00:46:48 d2.utils.events]: \u001B[0m eta: 0:49:53  iter: 419  total_loss: 1.942  loss_cls: 0.8708  loss_box_reg: 0.8752  loss_rpn_cls: 0.02104  loss_rpn_loc: 0.1784    time: 6.8802  last_time: 5.9591  data_time: 0.0011  last_data_time: 0.0009   lr: 1.1928e-06  \n",
      "\u001B[32m[07/18 00:48:40 d2.utils.events]: \u001B[0m eta: 0:48:38  iter: 439  total_loss: 1.943  loss_cls: 0.8619  loss_box_reg: 0.8758  loss_rpn_cls: 0.01871  loss_rpn_loc: 0.1789    time: 6.8236  last_time: 5.6901  data_time: 0.0011  last_data_time: 0.0011   lr: 1.2378e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:49:37 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:49:37 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:49:37 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:49:37 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:49:37 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:49:37 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:49:56 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.5444 s/iter. Eval: 0.0003 s/iter. Total: 1.5451 s/iter. ETA=0:01:45\n",
      "\u001B[32m[07/18 00:50:01 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0004 s/iter. Inference: 1.4747 s/iter. Eval: 0.0002 s/iter. Total: 1.4755 s/iter. ETA=0:01:34\n",
      "\u001B[32m[07/18 00:50:07 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0004 s/iter. Inference: 1.4480 s/iter. Eval: 0.0002 s/iter. Total: 1.4488 s/iter. ETA=0:01:26\n",
      "\u001B[32m[07/18 00:50:13 d2.evaluation.evaluator]: \u001B[0mInference done 24/79. Dataloading: 0.0005 s/iter. Inference: 1.4152 s/iter. Eval: 0.0002 s/iter. Total: 1.4160 s/iter. ETA=0:01:17\n",
      "\u001B[32m[07/18 00:50:19 d2.evaluation.evaluator]: \u001B[0mInference done 28/79. Dataloading: 0.0005 s/iter. Inference: 1.4259 s/iter. Eval: 0.0002 s/iter. Total: 1.4267 s/iter. ETA=0:01:12\n",
      "\u001B[32m[07/18 00:50:24 d2.evaluation.evaluator]: \u001B[0mInference done 32/79. Dataloading: 0.0005 s/iter. Inference: 1.4048 s/iter. Eval: 0.0002 s/iter. Total: 1.4056 s/iter. ETA=0:01:06\n",
      "\u001B[32m[07/18 00:50:30 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0005 s/iter. Inference: 1.3989 s/iter. Eval: 0.0002 s/iter. Total: 1.3997 s/iter. ETA=0:01:00\n",
      "\u001B[32m[07/18 00:50:35 d2.evaluation.evaluator]: \u001B[0mInference done 40/79. Dataloading: 0.0005 s/iter. Inference: 1.3973 s/iter. Eval: 0.0002 s/iter. Total: 1.3981 s/iter. ETA=0:00:54\n",
      "\u001B[32m[07/18 00:50:41 d2.evaluation.evaluator]: \u001B[0mInference done 44/79. Dataloading: 0.0005 s/iter. Inference: 1.3976 s/iter. Eval: 0.0002 s/iter. Total: 1.3984 s/iter. ETA=0:00:48\n",
      "\u001B[32m[07/18 00:50:46 d2.evaluation.evaluator]: \u001B[0mInference done 48/79. Dataloading: 0.0005 s/iter. Inference: 1.3897 s/iter. Eval: 0.0002 s/iter. Total: 1.3905 s/iter. ETA=0:00:43\n",
      "\u001B[32m[07/18 00:50:51 d2.evaluation.evaluator]: \u001B[0mInference done 52/79. Dataloading: 0.0005 s/iter. Inference: 1.3842 s/iter. Eval: 0.0002 s/iter. Total: 1.3850 s/iter. ETA=0:00:37\n",
      "\u001B[32m[07/18 00:50:57 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0005 s/iter. Inference: 1.3869 s/iter. Eval: 0.0002 s/iter. Total: 1.3878 s/iter. ETA=0:00:31\n",
      "\u001B[32m[07/18 00:51:02 d2.evaluation.evaluator]: \u001B[0mInference done 60/79. Dataloading: 0.0005 s/iter. Inference: 1.3842 s/iter. Eval: 0.0002 s/iter. Total: 1.3851 s/iter. ETA=0:00:26\n",
      "\u001B[32m[07/18 00:51:08 d2.evaluation.evaluator]: \u001B[0mInference done 64/79. Dataloading: 0.0005 s/iter. Inference: 1.3856 s/iter. Eval: 0.0002 s/iter. Total: 1.3864 s/iter. ETA=0:00:20\n",
      "\u001B[32m[07/18 00:51:15 d2.evaluation.evaluator]: \u001B[0mInference done 69/79. Dataloading: 0.0005 s/iter. Inference: 1.3790 s/iter. Eval: 0.0002 s/iter. Total: 1.3799 s/iter. ETA=0:00:13\n",
      "\u001B[32m[07/18 00:51:20 d2.evaluation.evaluator]: \u001B[0mInference done 73/79. Dataloading: 0.0005 s/iter. Inference: 1.3750 s/iter. Eval: 0.0002 s/iter. Total: 1.3759 s/iter. ETA=0:00:08\n",
      "\u001B[32m[07/18 00:51:25 d2.evaluation.evaluator]: \u001B[0mInference done 77/79. Dataloading: 0.0005 s/iter. Inference: 1.3693 s/iter. Eval: 0.0002 s/iter. Total: 1.3701 s/iter. ETA=0:00:02\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:41.782715 (1.375442 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:41 (1.369435 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.044\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.012\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.027\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.115\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.115\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 1.070 | 4.361  | 0.056  |  nan  |  nan  | 1.213 |\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.017 | book       | 2.122 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:51:28 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:51:28 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:51:28 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:51:28 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:51:28 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:51:28 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:51:47 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.5380 s/iter. Eval: 0.0002 s/iter. Total: 1.5386 s/iter. ETA=0:01:44\n",
      "\u001B[32m[07/18 00:51:53 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0005 s/iter. Inference: 1.5244 s/iter. Eval: 0.0002 s/iter. Total: 1.5252 s/iter. ETA=0:01:37\n",
      "\u001B[32m[07/18 00:51:59 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0005 s/iter. Inference: 1.4804 s/iter. Eval: 0.0002 s/iter. Total: 1.4812 s/iter. ETA=0:01:28\n",
      "\u001B[32m[07/18 00:52:04 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0005 s/iter. Inference: 1.4337 s/iter. Eval: 0.0002 s/iter. Total: 1.4345 s/iter. ETA=0:01:20\n",
      "\u001B[32m[07/18 00:52:09 d2.evaluation.evaluator]: \u001B[0mInference done 27/79. Dataloading: 0.0005 s/iter. Inference: 1.4184 s/iter. Eval: 0.0002 s/iter. Total: 1.4192 s/iter. ETA=0:01:13\n",
      "\u001B[32m[07/18 00:52:15 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.4253 s/iter. Eval: 0.0002 s/iter. Total: 1.4261 s/iter. ETA=0:01:08\n",
      "\u001B[32m[07/18 00:52:20 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0005 s/iter. Inference: 1.4223 s/iter. Eval: 0.0002 s/iter. Total: 1.4231 s/iter. ETA=0:01:02\n",
      "\u001B[32m[07/18 00:52:26 d2.evaluation.evaluator]: \u001B[0mInference done 39/79. Dataloading: 0.0005 s/iter. Inference: 1.4177 s/iter. Eval: 0.0002 s/iter. Total: 1.4185 s/iter. ETA=0:00:56\n",
      "\u001B[32m[07/18 00:52:31 d2.evaluation.evaluator]: \u001B[0mInference done 43/79. Dataloading: 0.0005 s/iter. Inference: 1.4121 s/iter. Eval: 0.0002 s/iter. Total: 1.4129 s/iter. ETA=0:00:50\n",
      "\u001B[32m[07/18 00:52:36 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0005 s/iter. Inference: 1.3968 s/iter. Eval: 0.0002 s/iter. Total: 1.3976 s/iter. ETA=0:00:44\n",
      "\u001B[32m[07/18 00:52:42 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0005 s/iter. Inference: 1.3976 s/iter. Eval: 0.0002 s/iter. Total: 1.3984 s/iter. ETA=0:00:39\n",
      "\u001B[32m[07/18 00:52:48 d2.evaluation.evaluator]: \u001B[0mInference done 55/79. Dataloading: 0.0005 s/iter. Inference: 1.3979 s/iter. Eval: 0.0026 s/iter. Total: 1.4010 s/iter. ETA=0:00:33\n",
      "\u001B[32m[07/18 00:52:54 d2.evaluation.evaluator]: \u001B[0mInference done 60/79. Dataloading: 0.0005 s/iter. Inference: 1.3908 s/iter. Eval: 0.0023 s/iter. Total: 1.3938 s/iter. ETA=0:00:26\n",
      "\u001B[32m[07/18 00:53:01 d2.evaluation.evaluator]: \u001B[0mInference done 65/79. Dataloading: 0.0005 s/iter. Inference: 1.3847 s/iter. Eval: 0.0021 s/iter. Total: 1.3875 s/iter. ETA=0:00:19\n",
      "\u001B[32m[07/18 00:53:06 d2.evaluation.evaluator]: \u001B[0mInference done 69/79. Dataloading: 0.0005 s/iter. Inference: 1.3814 s/iter. Eval: 0.0020 s/iter. Total: 1.3840 s/iter. ETA=0:00:13\n",
      "\u001B[32m[07/18 00:53:11 d2.evaluation.evaluator]: \u001B[0mInference done 73/79. Dataloading: 0.0005 s/iter. Inference: 1.3741 s/iter. Eval: 0.0019 s/iter. Total: 1.3766 s/iter. ETA=0:00:08\n",
      "\u001B[32m[07/18 00:53:17 d2.evaluation.evaluator]: \u001B[0mInference done 77/79. Dataloading: 0.0005 s/iter. Inference: 1.3758 s/iter. Eval: 0.0018 s/iter. Total: 1.3782 s/iter. ETA=0:00:02\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:42.472105 (1.384758 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:41 (1.377610 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.044\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.012\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.027\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.115\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.115\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 1.070 | 4.361  | 0.056  |  nan  |  nan  | 1.213 |\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 00:53:20 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.017 | book       | 2.122 |\n",
      "\u001B[32m[07/18 00:54:20 d2.utils.events]: \u001B[0m eta: 0:47:24  iter: 459  total_loss: 1.939  loss_cls: 0.8517  loss_box_reg: 0.8829  loss_rpn_cls: 0.01822  loss_rpn_loc: 0.1805    time: 7.0214  last_time: 6.0436  data_time: 0.0011  last_data_time: 0.0009   lr: 1.2828e-06  \n",
      "\u001B[32m[07/18 00:56:15 d2.utils.events]: \u001B[0m eta: 0:46:09  iter: 479  total_loss: 1.933  loss_cls: 0.8434  loss_box_reg: 0.8825  loss_rpn_cls: 0.01684  loss_rpn_loc: 0.1738    time: 6.9695  last_time: 5.0712  data_time: 0.0015  last_data_time: 0.0017   lr: 1.3278e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 00:58:12 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 00:58:12 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 00:58:12 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 00:58:12 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 00:58:12 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 00:58:12 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 00:58:29 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.4072 s/iter. Eval: 0.0001 s/iter. Total: 1.4076 s/iter. ETA=0:01:35\n",
      "\u001B[32m[07/18 00:58:35 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0004 s/iter. Inference: 1.3900 s/iter. Eval: 0.0001 s/iter. Total: 1.3906 s/iter. ETA=0:01:28\n",
      "\u001B[32m[07/18 00:58:41 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0004 s/iter. Inference: 1.4040 s/iter. Eval: 0.0002 s/iter. Total: 1.4047 s/iter. ETA=0:01:24\n",
      "\u001B[32m[07/18 00:58:46 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0004 s/iter. Inference: 1.4027 s/iter. Eval: 0.0002 s/iter. Total: 1.4034 s/iter. ETA=0:01:18\n",
      "\u001B[32m[07/18 00:58:52 d2.evaluation.evaluator]: \u001B[0mInference done 27/79. Dataloading: 0.0004 s/iter. Inference: 1.3856 s/iter. Eval: 0.0002 s/iter. Total: 1.3863 s/iter. ETA=0:01:12\n",
      "\u001B[32m[07/18 00:58:57 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0004 s/iter. Inference: 1.3791 s/iter. Eval: 0.0002 s/iter. Total: 1.3799 s/iter. ETA=0:01:06\n",
      "\u001B[32m[07/18 00:59:03 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0004 s/iter. Inference: 1.3850 s/iter. Eval: 0.0002 s/iter. Total: 1.3858 s/iter. ETA=0:01:00\n",
      "\u001B[32m[07/18 00:59:08 d2.evaluation.evaluator]: \u001B[0mInference done 39/79. Dataloading: 0.0004 s/iter. Inference: 1.3825 s/iter. Eval: 0.0002 s/iter. Total: 1.3833 s/iter. ETA=0:00:55\n",
      "\u001B[32m[07/18 00:59:13 d2.evaluation.evaluator]: \u001B[0mInference done 43/79. Dataloading: 0.0004 s/iter. Inference: 1.3743 s/iter. Eval: 0.0002 s/iter. Total: 1.3751 s/iter. ETA=0:00:49\n",
      "\u001B[32m[07/18 00:59:18 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0004 s/iter. Inference: 1.3639 s/iter. Eval: 0.0002 s/iter. Total: 1.3647 s/iter. ETA=0:00:43\n",
      "\u001B[32m[07/18 00:59:24 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0004 s/iter. Inference: 1.3694 s/iter. Eval: 0.0002 s/iter. Total: 1.3701 s/iter. ETA=0:00:38\n",
      "\u001B[32m[07/18 00:59:30 d2.evaluation.evaluator]: \u001B[0mInference done 55/79. Dataloading: 0.0004 s/iter. Inference: 1.3730 s/iter. Eval: 0.0002 s/iter. Total: 1.3738 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 00:59:35 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0004 s/iter. Inference: 1.3755 s/iter. Eval: 0.0002 s/iter. Total: 1.3763 s/iter. ETA=0:00:27\n",
      "\u001B[32m[07/18 00:59:42 d2.evaluation.evaluator]: \u001B[0mInference done 64/79. Dataloading: 0.0004 s/iter. Inference: 1.3717 s/iter. Eval: 0.0002 s/iter. Total: 1.3725 s/iter. ETA=0:00:20\n",
      "\u001B[32m[07/18 00:59:48 d2.evaluation.evaluator]: \u001B[0mInference done 68/79. Dataloading: 0.0004 s/iter. Inference: 1.3737 s/iter. Eval: 0.0002 s/iter. Total: 1.3745 s/iter. ETA=0:00:15\n",
      "\u001B[32m[07/18 00:59:53 d2.evaluation.evaluator]: \u001B[0mInference done 72/79. Dataloading: 0.0004 s/iter. Inference: 1.3665 s/iter. Eval: 0.0002 s/iter. Total: 1.3673 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 00:59:58 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0004 s/iter. Inference: 1.3673 s/iter. Eval: 0.0002 s/iter. Total: 1.3681 s/iter. ETA=0:00:04\n",
      "\u001B[32m[07/18 01:00:02 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:41.385330 (1.370072 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:00:02 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:40 (1.363823 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:00:02 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:00:02 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:00:02 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:00:02 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:00:03 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 01:00:03 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:00:03 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.014\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.027\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.123\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.123\n",
      "\u001B[32m[07/18 01:00:03 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 1.199 | 4.702  | 0.075  |  nan  |  nan  | 1.353 |\n",
      "\u001B[32m[07/18 01:00:03 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:00:03 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.022 | book       | 2.376 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:00:03 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:00:03 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:00:03 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:00:03 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:00:03 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:00:03 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:00:20 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.3279 s/iter. Eval: 0.0002 s/iter. Total: 1.3284 s/iter. ETA=0:01:30\n",
      "\u001B[32m[07/18 01:00:26 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0004 s/iter. Inference: 1.3644 s/iter. Eval: 0.0002 s/iter. Total: 1.3651 s/iter. ETA=0:01:27\n",
      "\u001B[32m[07/18 01:00:31 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0005 s/iter. Inference: 1.3589 s/iter. Eval: 0.0002 s/iter. Total: 1.3596 s/iter. ETA=0:01:21\n",
      "\u001B[32m[07/18 01:00:37 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0004 s/iter. Inference: 1.3676 s/iter. Eval: 0.0002 s/iter. Total: 1.3683 s/iter. ETA=0:01:16\n",
      "\u001B[32m[07/18 01:00:42 d2.evaluation.evaluator]: \u001B[0mInference done 27/79. Dataloading: 0.0004 s/iter. Inference: 1.3525 s/iter. Eval: 0.0002 s/iter. Total: 1.3532 s/iter. ETA=0:01:10\n",
      "\u001B[32m[07/18 01:00:48 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0004 s/iter. Inference: 1.3692 s/iter. Eval: 0.0002 s/iter. Total: 1.3700 s/iter. ETA=0:01:05\n",
      "\u001B[32m[07/18 01:00:53 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0004 s/iter. Inference: 1.3624 s/iter. Eval: 0.0002 s/iter. Total: 1.3632 s/iter. ETA=0:00:59\n",
      "\u001B[32m[07/18 01:00:58 d2.evaluation.evaluator]: \u001B[0mInference done 39/79. Dataloading: 0.0004 s/iter. Inference: 1.3511 s/iter. Eval: 0.0002 s/iter. Total: 1.3518 s/iter. ETA=0:00:54\n",
      "\u001B[32m[07/18 01:01:04 d2.evaluation.evaluator]: \u001B[0mInference done 43/79. Dataloading: 0.0005 s/iter. Inference: 1.3538 s/iter. Eval: 0.0002 s/iter. Total: 1.3546 s/iter. ETA=0:00:48\n",
      "\u001B[32m[07/18 01:01:09 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0005 s/iter. Inference: 1.3575 s/iter. Eval: 0.0002 s/iter. Total: 1.3583 s/iter. ETA=0:00:43\n",
      "\u001B[32m[07/18 01:01:14 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0005 s/iter. Inference: 1.3501 s/iter. Eval: 0.0002 s/iter. Total: 1.3508 s/iter. ETA=0:00:37\n",
      "\u001B[32m[07/18 01:01:19 d2.evaluation.evaluator]: \u001B[0mInference done 54/79. Dataloading: 0.0005 s/iter. Inference: 1.3742 s/iter. Eval: 0.0002 s/iter. Total: 1.3750 s/iter. ETA=0:00:34\n",
      "\u001B[32m[07/18 01:01:25 d2.evaluation.evaluator]: \u001B[0mInference done 58/79. Dataloading: 0.0004 s/iter. Inference: 1.3728 s/iter. Eval: 0.0002 s/iter. Total: 1.3735 s/iter. ETA=0:00:28\n",
      "\u001B[32m[07/18 01:01:30 d2.evaluation.evaluator]: \u001B[0mInference done 62/79. Dataloading: 0.0004 s/iter. Inference: 1.3664 s/iter. Eval: 0.0002 s/iter. Total: 1.3671 s/iter. ETA=0:00:23\n",
      "\u001B[32m[07/18 01:01:35 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0004 s/iter. Inference: 1.3654 s/iter. Eval: 0.0002 s/iter. Total: 1.3662 s/iter. ETA=0:00:17\n",
      "\u001B[32m[07/18 01:01:41 d2.evaluation.evaluator]: \u001B[0mInference done 70/79. Dataloading: 0.0004 s/iter. Inference: 1.3690 s/iter. Eval: 0.0002 s/iter. Total: 1.3698 s/iter. ETA=0:00:12\n",
      "\u001B[32m[07/18 01:01:46 d2.evaluation.evaluator]: \u001B[0mInference done 74/79. Dataloading: 0.0004 s/iter. Inference: 1.3644 s/iter. Eval: 0.0002 s/iter. Total: 1.3651 s/iter. ETA=0:00:06\n",
      "\u001B[32m[07/18 01:01:52 d2.evaluation.evaluator]: \u001B[0mInference done 78/79. Dataloading: 0.0004 s/iter. Inference: 1.3648 s/iter. Eval: 0.0002 s/iter. Total: 1.3656 s/iter. ETA=0:00:01\n",
      "\u001B[32m[07/18 01:01:53 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:41.374421 (1.369925 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:01:53 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:40 (1.363283 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:01:53 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:01:53 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:01:53 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:01:53 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:01:54 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 01:01:54 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:01:54 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.014\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.027\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.123\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.123\n",
      "\u001B[32m[07/18 01:01:54 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 1.199 | 4.702  | 0.075  |  nan  |  nan  | 1.353 |\n",
      "\u001B[32m[07/18 01:01:54 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:01:54 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.022 | book       | 2.376 |\n",
      "\u001B[32m[07/18 01:01:54 d2.utils.events]: \u001B[0m eta: 0:44:38  iter: 499  total_loss: 1.909  loss_cls: 0.8346  loss_box_reg: 0.8809  loss_rpn_cls: 0.01661  loss_rpn_loc: 0.1779    time: 7.1460  last_time: 116.4864  data_time: 0.0012  last_data_time: 0.0011   lr: 1.3728e-06  \n",
      "\u001B[32m[07/18 01:03:51 d2.utils.events]: \u001B[0m eta: 0:43:07  iter: 519  total_loss: 1.884  loss_cls: 0.8284  loss_box_reg: 0.8433  loss_rpn_cls: 0.01727  loss_rpn_loc: 0.1795    time: 7.0970  last_time: 5.8200  data_time: 0.0013  last_data_time: 0.0013   lr: 1.4178e-06  \n",
      "\u001B[32m[07/18 01:05:47 d2.utils.events]: \u001B[0m eta: 0:41:34  iter: 539  total_loss: 1.902  loss_cls: 0.8179  loss_box_reg: 0.8934  loss_rpn_cls: 0.02025  loss_rpn_loc: 0.179    time: 7.0483  last_time: 5.7691  data_time: 0.0012  last_data_time: 0.0014   lr: 1.4628e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:06:45 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:06:45 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:06:45 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:06:45 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:06:45 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:06:45 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:07:02 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.3917 s/iter. Eval: 0.0001 s/iter. Total: 1.3922 s/iter. ETA=0:01:34\n",
      "\u001B[32m[07/18 01:07:08 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0004 s/iter. Inference: 1.3918 s/iter. Eval: 0.0002 s/iter. Total: 1.3926 s/iter. ETA=0:01:29\n",
      "\u001B[32m[07/18 01:07:13 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0004 s/iter. Inference: 1.3972 s/iter. Eval: 0.0002 s/iter. Total: 1.3979 s/iter. ETA=0:01:23\n",
      "\u001B[32m[07/18 01:07:19 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0004 s/iter. Inference: 1.3873 s/iter. Eval: 0.0002 s/iter. Total: 1.3881 s/iter. ETA=0:01:17\n",
      "\u001B[32m[07/18 01:07:24 d2.evaluation.evaluator]: \u001B[0mInference done 27/79. Dataloading: 0.0005 s/iter. Inference: 1.3623 s/iter. Eval: 0.0002 s/iter. Total: 1.3631 s/iter. ETA=0:01:10\n",
      "\u001B[32m[07/18 01:07:29 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.3660 s/iter. Eval: 0.0002 s/iter. Total: 1.3669 s/iter. ETA=0:01:05\n",
      "\u001B[32m[07/18 01:07:35 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0005 s/iter. Inference: 1.3647 s/iter. Eval: 0.0002 s/iter. Total: 1.3655 s/iter. ETA=0:01:00\n",
      "\u001B[32m[07/18 01:07:40 d2.evaluation.evaluator]: \u001B[0mInference done 39/79. Dataloading: 0.0005 s/iter. Inference: 1.3662 s/iter. Eval: 0.0002 s/iter. Total: 1.3670 s/iter. ETA=0:00:54\n",
      "\u001B[32m[07/18 01:07:46 d2.evaluation.evaluator]: \u001B[0mInference done 43/79. Dataloading: 0.0005 s/iter. Inference: 1.3714 s/iter. Eval: 0.0002 s/iter. Total: 1.3723 s/iter. ETA=0:00:49\n",
      "\u001B[32m[07/18 01:07:52 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0005 s/iter. Inference: 1.3759 s/iter. Eval: 0.0002 s/iter. Total: 1.3768 s/iter. ETA=0:00:44\n",
      "\u001B[32m[07/18 01:07:57 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0005 s/iter. Inference: 1.3724 s/iter. Eval: 0.0002 s/iter. Total: 1.3733 s/iter. ETA=0:00:38\n",
      "\u001B[32m[07/18 01:08:02 d2.evaluation.evaluator]: \u001B[0mInference done 55/79. Dataloading: 0.0005 s/iter. Inference: 1.3680 s/iter. Eval: 0.0002 s/iter. Total: 1.3688 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 01:08:08 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0005 s/iter. Inference: 1.3748 s/iter. Eval: 0.0002 s/iter. Total: 1.3757 s/iter. ETA=0:00:27\n",
      "\u001B[32m[07/18 01:08:14 d2.evaluation.evaluator]: \u001B[0mInference done 63/79. Dataloading: 0.0005 s/iter. Inference: 1.3751 s/iter. Eval: 0.0002 s/iter. Total: 1.3760 s/iter. ETA=0:00:22\n",
      "\u001B[32m[07/18 01:08:19 d2.evaluation.evaluator]: \u001B[0mInference done 67/79. Dataloading: 0.0005 s/iter. Inference: 1.3785 s/iter. Eval: 0.0002 s/iter. Total: 1.3794 s/iter. ETA=0:00:16\n",
      "\u001B[32m[07/18 01:08:25 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0005 s/iter. Inference: 1.3765 s/iter. Eval: 0.0002 s/iter. Total: 1.3773 s/iter. ETA=0:00:11\n",
      "\u001B[32m[07/18 01:08:30 d2.evaluation.evaluator]: \u001B[0mInference done 75/79. Dataloading: 0.0005 s/iter. Inference: 1.3720 s/iter. Eval: 0.0002 s/iter. Total: 1.3729 s/iter. ETA=0:00:05\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.evaluator]: \u001B[0mInference done 79/79. Dataloading: 0.0005 s/iter. Inference: 1.3738 s/iter. Eval: 0.0002 s/iter. Total: 1.3746 s/iter. ETA=0:00:00\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:42.025846 (1.378728 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:41 (1.373767 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.13s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.015\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.028\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.127\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 1.330 | 5.062  | 0.107  |  nan  |  nan  | 1.498 |\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.026 | book       | 2.634 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:08:36 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:08:36 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:08:36 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:08:36 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:08:36 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:08:36 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:08:56 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.5444 s/iter. Eval: 0.0002 s/iter. Total: 1.5449 s/iter. ETA=0:01:45\n",
      "\u001B[32m[07/18 01:09:01 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0004 s/iter. Inference: 1.5004 s/iter. Eval: 0.0002 s/iter. Total: 1.5011 s/iter. ETA=0:01:36\n",
      "\u001B[32m[07/18 01:09:07 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0004 s/iter. Inference: 1.4466 s/iter. Eval: 0.0001 s/iter. Total: 1.4473 s/iter. ETA=0:01:26\n",
      "\u001B[32m[07/18 01:09:13 d2.evaluation.evaluator]: \u001B[0mInference done 22/79. Dataloading: 0.0005 s/iter. Inference: 1.5476 s/iter. Eval: 0.0002 s/iter. Total: 1.5484 s/iter. ETA=0:01:28\n",
      "\u001B[32m[07/18 01:09:19 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0005 s/iter. Inference: 1.5361 s/iter. Eval: 0.0002 s/iter. Total: 1.5369 s/iter. ETA=0:01:21\n",
      "\u001B[32m[07/18 01:09:24 d2.evaluation.evaluator]: \u001B[0mInference done 30/79. Dataloading: 0.0005 s/iter. Inference: 1.5177 s/iter. Eval: 0.0002 s/iter. Total: 1.5185 s/iter. ETA=0:01:14\n",
      "\u001B[32m[07/18 01:09:30 d2.evaluation.evaluator]: \u001B[0mInference done 34/79. Dataloading: 0.0005 s/iter. Inference: 1.4988 s/iter. Eval: 0.0002 s/iter. Total: 1.4996 s/iter. ETA=0:01:07\n",
      "\u001B[32m[07/18 01:09:35 d2.evaluation.evaluator]: \u001B[0mInference done 38/79. Dataloading: 0.0005 s/iter. Inference: 1.4792 s/iter. Eval: 0.0002 s/iter. Total: 1.4800 s/iter. ETA=0:01:00\n",
      "\u001B[32m[07/18 01:09:41 d2.evaluation.evaluator]: \u001B[0mInference done 43/79. Dataloading: 0.0005 s/iter. Inference: 1.4493 s/iter. Eval: 0.0002 s/iter. Total: 1.4501 s/iter. ETA=0:00:52\n",
      "\u001B[32m[07/18 01:09:47 d2.evaluation.evaluator]: \u001B[0mInference done 48/79. Dataloading: 0.0005 s/iter. Inference: 1.4201 s/iter. Eval: 0.0002 s/iter. Total: 1.4209 s/iter. ETA=0:00:44\n",
      "\u001B[32m[07/18 01:09:53 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0005 s/iter. Inference: 1.3976 s/iter. Eval: 0.0002 s/iter. Total: 1.3984 s/iter. ETA=0:00:36\n",
      "\u001B[32m[07/18 01:09:59 d2.evaluation.evaluator]: \u001B[0mInference done 58/79. Dataloading: 0.0005 s/iter. Inference: 1.3761 s/iter. Eval: 0.0002 s/iter. Total: 1.3769 s/iter. ETA=0:00:28\n",
      "\u001B[32m[07/18 01:10:05 d2.evaluation.evaluator]: \u001B[0mInference done 63/79. Dataloading: 0.0005 s/iter. Inference: 1.3613 s/iter. Eval: 0.0002 s/iter. Total: 1.3621 s/iter. ETA=0:00:21\n",
      "\u001B[32m[07/18 01:10:11 d2.evaluation.evaluator]: \u001B[0mInference done 68/79. Dataloading: 0.0005 s/iter. Inference: 1.3459 s/iter. Eval: 0.0002 s/iter. Total: 1.3467 s/iter. ETA=0:00:14\n",
      "\u001B[32m[07/18 01:10:16 d2.evaluation.evaluator]: \u001B[0mInference done 72/79. Dataloading: 0.0005 s/iter. Inference: 1.3414 s/iter. Eval: 0.0002 s/iter. Total: 1.3422 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 01:10:22 d2.evaluation.evaluator]: \u001B[0mInference done 77/79. Dataloading: 0.0005 s/iter. Inference: 1.3312 s/iter. Eval: 0.0002 s/iter. Total: 1.3320 s/iter. ETA=0:00:02\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:38.538763 (1.331605 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:38 (1.326478 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.015\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.028\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.127\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 1.330 | 5.062  | 0.107  |  nan  |  nan  | 1.498 |\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:10:25 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.026 | book       | 2.634 |\n",
      "\u001B[32m[07/18 01:11:24 d2.utils.events]: \u001B[0m eta: 0:40:03  iter: 559  total_loss: 1.912  loss_cls: 0.8084  loss_box_reg: 0.9076  loss_rpn_cls: 0.01906  loss_rpn_loc: 0.1781    time: 7.2042  last_time: 4.9702  data_time: 0.0011  last_data_time: 0.0013   lr: 1.5078e-06  \n",
      "\u001B[32m[07/18 01:13:12 d2.utils.events]: \u001B[0m eta: 0:38:13  iter: 579  total_loss: 1.871  loss_cls: 0.8038  loss_box_reg: 0.8746  loss_rpn_cls: 0.01874  loss_rpn_loc: 0.1779    time: 7.1422  last_time: 4.9286  data_time: 0.0014  last_data_time: 0.0011   lr: 1.5528e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:14:56 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:14:56 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:14:56 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:14:56 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:14:56 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:14:56 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:15:12 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.1902 s/iter. Eval: 0.0001 s/iter. Total: 1.1907 s/iter. ETA=0:01:20\n",
      "\u001B[32m[07/18 01:15:18 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0003 s/iter. Inference: 1.2065 s/iter. Eval: 0.0002 s/iter. Total: 1.2071 s/iter. ETA=0:01:16\n",
      "\u001B[32m[07/18 01:15:24 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0004 s/iter. Inference: 1.1996 s/iter. Eval: 0.0002 s/iter. Total: 1.2003 s/iter. ETA=0:01:09\n",
      "\u001B[32m[07/18 01:15:29 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0004 s/iter. Inference: 1.1889 s/iter. Eval: 0.0002 s/iter. Total: 1.1896 s/iter. ETA=0:01:03\n",
      "\u001B[32m[07/18 01:15:35 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.1816 s/iter. Eval: 0.0002 s/iter. Total: 1.1824 s/iter. ETA=0:00:56\n",
      "\u001B[32m[07/18 01:15:41 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0005 s/iter. Inference: 1.1802 s/iter. Eval: 0.0002 s/iter. Total: 1.1809 s/iter. ETA=0:00:50\n",
      "\u001B[32m[07/18 01:15:47 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0005 s/iter. Inference: 1.1811 s/iter. Eval: 0.0002 s/iter. Total: 1.1819 s/iter. ETA=0:00:44\n",
      "\u001B[32m[07/18 01:15:53 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0004 s/iter. Inference: 1.1801 s/iter. Eval: 0.0002 s/iter. Total: 1.1808 s/iter. ETA=0:00:38\n",
      "\u001B[32m[07/18 01:15:59 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0004 s/iter. Inference: 1.1776 s/iter. Eval: 0.0002 s/iter. Total: 1.1783 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 01:16:04 d2.evaluation.evaluator]: \u001B[0mInference done 55/79. Dataloading: 0.0005 s/iter. Inference: 1.1949 s/iter. Eval: 0.0002 s/iter. Total: 1.1957 s/iter. ETA=0:00:28\n",
      "\u001B[32m[07/18 01:16:09 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0005 s/iter. Inference: 1.2005 s/iter. Eval: 0.0002 s/iter. Total: 1.2012 s/iter. ETA=0:00:24\n",
      "\u001B[32m[07/18 01:16:15 d2.evaluation.evaluator]: \u001B[0mInference done 64/79. Dataloading: 0.0005 s/iter. Inference: 1.1971 s/iter. Eval: 0.0002 s/iter. Total: 1.1978 s/iter. ETA=0:00:17\n",
      "\u001B[32m[07/18 01:16:21 d2.evaluation.evaluator]: \u001B[0mInference done 69/79. Dataloading: 0.0005 s/iter. Inference: 1.1984 s/iter. Eval: 0.0002 s/iter. Total: 1.1992 s/iter. ETA=0:00:11\n",
      "\u001B[32m[07/18 01:16:27 d2.evaluation.evaluator]: \u001B[0mInference done 73/79. Dataloading: 0.0005 s/iter. Inference: 1.2064 s/iter. Eval: 0.0002 s/iter. Total: 1.2072 s/iter. ETA=0:00:07\n",
      "\u001B[32m[07/18 01:16:32 d2.evaluation.evaluator]: \u001B[0mInference done 77/79. Dataloading: 0.0005 s/iter. Inference: 1.2098 s/iter. Eval: 0.0002 s/iter. Total: 1.2106 s/iter. ETA=0:00:02\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:30.169434 (1.218506 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:29 (1.211653 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.015\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.056\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.017\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.029\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.135\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.135\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 1.527 | 5.640  | 0.158  |  nan  |  nan  | 1.714 |\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.040 | book       | 3.013 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:16:35 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:16:35 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:16:35 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:16:35 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:16:35 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:16:35 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:16:53 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.2644 s/iter. Eval: 0.0002 s/iter. Total: 1.2650 s/iter. ETA=0:01:26\n",
      "\u001B[32m[07/18 01:17:00 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0004 s/iter. Inference: 1.2753 s/iter. Eval: 0.0002 s/iter. Total: 1.2761 s/iter. ETA=0:01:20\n",
      "\u001B[32m[07/18 01:17:05 d2.evaluation.evaluator]: \u001B[0mInference done 20/79. Dataloading: 0.0004 s/iter. Inference: 1.3116 s/iter. Eval: 0.0002 s/iter. Total: 1.3123 s/iter. ETA=0:01:17\n",
      "\u001B[32m[07/18 01:17:11 d2.evaluation.evaluator]: \u001B[0mInference done 25/79. Dataloading: 0.0004 s/iter. Inference: 1.2967 s/iter. Eval: 0.0002 s/iter. Total: 1.2974 s/iter. ETA=0:01:10\n",
      "\u001B[32m[07/18 01:17:18 d2.evaluation.evaluator]: \u001B[0mInference done 30/79. Dataloading: 0.0004 s/iter. Inference: 1.2816 s/iter. Eval: 0.0002 s/iter. Total: 1.2823 s/iter. ETA=0:01:02\n",
      "\u001B[32m[07/18 01:17:23 d2.evaluation.evaluator]: \u001B[0mInference done 34/79. Dataloading: 0.0004 s/iter. Inference: 1.2771 s/iter. Eval: 0.0002 s/iter. Total: 1.2779 s/iter. ETA=0:00:57\n",
      "\u001B[32m[07/18 01:17:29 d2.evaluation.evaluator]: \u001B[0mInference done 39/79. Dataloading: 0.0004 s/iter. Inference: 1.2726 s/iter. Eval: 0.0002 s/iter. Total: 1.2734 s/iter. ETA=0:00:50\n",
      "\u001B[32m[07/18 01:17:35 d2.evaluation.evaluator]: \u001B[0mInference done 44/79. Dataloading: 0.0004 s/iter. Inference: 1.2723 s/iter. Eval: 0.0002 s/iter. Total: 1.2731 s/iter. ETA=0:00:44\n",
      "\u001B[32m[07/18 01:17:41 d2.evaluation.evaluator]: \u001B[0mInference done 49/79. Dataloading: 0.0004 s/iter. Inference: 1.2661 s/iter. Eval: 0.0002 s/iter. Total: 1.2668 s/iter. ETA=0:00:38\n",
      "\u001B[32m[07/18 01:17:46 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0005 s/iter. Inference: 1.2677 s/iter. Eval: 0.0002 s/iter. Total: 1.2685 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 01:17:52 d2.evaluation.evaluator]: \u001B[0mInference done 58/79. Dataloading: 0.0005 s/iter. Inference: 1.2607 s/iter. Eval: 0.0002 s/iter. Total: 1.2615 s/iter. ETA=0:00:26\n",
      "\u001B[32m[07/18 01:17:58 d2.evaluation.evaluator]: \u001B[0mInference done 63/79. Dataloading: 0.0005 s/iter. Inference: 1.2532 s/iter. Eval: 0.0002 s/iter. Total: 1.2541 s/iter. ETA=0:00:20\n",
      "\u001B[32m[07/18 01:18:04 d2.evaluation.evaluator]: \u001B[0mInference done 68/79. Dataloading: 0.0005 s/iter. Inference: 1.2477 s/iter. Eval: 0.0002 s/iter. Total: 1.2486 s/iter. ETA=0:00:13\n",
      "\u001B[32m[07/18 01:18:10 d2.evaluation.evaluator]: \u001B[0mInference done 73/79. Dataloading: 0.0005 s/iter. Inference: 1.2443 s/iter. Eval: 0.0002 s/iter. Total: 1.2452 s/iter. ETA=0:00:07\n",
      "\u001B[32m[07/18 01:18:16 d2.evaluation.evaluator]: \u001B[0mInference done 78/79. Dataloading: 0.0005 s/iter. Inference: 1.2437 s/iter. Eval: 0.0002 s/iter. Total: 1.2445 s/iter. ETA=0:00:01\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:32.505108 (1.250069 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:31 (1.240939 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.015\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.056\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.017\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.029\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.135\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.135\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 1.527 | 5.640  | 0.158  |  nan  |  nan  | 1.714 |\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:18:18 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.040 | book       | 3.013 |\n",
      "\u001B[32m[07/18 01:18:18 d2.utils.events]: \u001B[0m eta: 0:36:24  iter: 599  total_loss: 1.853  loss_cls: 0.7921  loss_box_reg: 0.8707  loss_rpn_cls: 0.01911  loss_rpn_loc: 0.1745    time: 7.2427  last_time: 103.7900  data_time: 0.0012  last_data_time: 0.0011   lr: 1.5978e-06  \n",
      "\u001B[32m[07/18 01:20:05 d2.utils.events]: \u001B[0m eta: 0:34:34  iter: 619  total_loss: 1.821  loss_cls: 0.7791  loss_box_reg: 0.8535  loss_rpn_cls: 0.02051  loss_rpn_loc: 0.1671    time: 7.1812  last_time: 5.4609  data_time: 0.0012  last_data_time: 0.0016   lr: 1.6428e-06  \n",
      "\u001B[32m[07/18 01:21:46 d2.utils.events]: \u001B[0m eta: 0:32:37  iter: 639  total_loss: 1.83  loss_cls: 0.7728  loss_box_reg: 0.8626  loss_rpn_cls: 0.02306  loss_rpn_loc: 0.1621    time: 7.1140  last_time: 4.4333  data_time: 0.0012  last_data_time: 0.0008   lr: 1.6878e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:22:40 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:22:40 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:22:40 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:22:40 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:22:40 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:22:40 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:22:56 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.1922 s/iter. Eval: 0.0002 s/iter. Total: 1.1928 s/iter. ETA=0:01:21\n",
      "\u001B[32m[07/18 01:23:02 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0004 s/iter. Inference: 1.1710 s/iter. Eval: 0.0002 s/iter. Total: 1.1716 s/iter. ETA=0:01:13\n",
      "\u001B[32m[07/18 01:23:07 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0004 s/iter. Inference: 1.1641 s/iter. Eval: 0.0002 s/iter. Total: 1.1648 s/iter. ETA=0:01:07\n",
      "\u001B[32m[07/18 01:23:13 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0004 s/iter. Inference: 1.1771 s/iter. Eval: 0.0002 s/iter. Total: 1.1779 s/iter. ETA=0:01:02\n",
      "\u001B[32m[07/18 01:23:19 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0004 s/iter. Inference: 1.1811 s/iter. Eval: 0.0002 s/iter. Total: 1.1819 s/iter. ETA=0:00:56\n",
      "\u001B[32m[07/18 01:23:25 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0004 s/iter. Inference: 1.1798 s/iter. Eval: 0.0002 s/iter. Total: 1.1805 s/iter. ETA=0:00:50\n",
      "\u001B[32m[07/18 01:23:31 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 1.1782 s/iter. Eval: 0.0002 s/iter. Total: 1.1790 s/iter. ETA=0:00:44\n",
      "\u001B[32m[07/18 01:23:37 d2.evaluation.evaluator]: \u001B[0mInference done 45/79. Dataloading: 0.0004 s/iter. Inference: 1.2119 s/iter. Eval: 0.0002 s/iter. Total: 1.2126 s/iter. ETA=0:00:41\n",
      "\u001B[32m[07/18 01:23:42 d2.evaluation.evaluator]: \u001B[0mInference done 49/79. Dataloading: 0.0004 s/iter. Inference: 1.2172 s/iter. Eval: 0.0002 s/iter. Total: 1.2179 s/iter. ETA=0:00:36\n",
      "\u001B[32m[07/18 01:23:47 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0005 s/iter. Inference: 1.2240 s/iter. Eval: 0.0002 s/iter. Total: 1.2248 s/iter. ETA=0:00:31\n",
      "\u001B[32m[07/18 01:23:53 d2.evaluation.evaluator]: \u001B[0mInference done 57/79. Dataloading: 0.0005 s/iter. Inference: 1.2308 s/iter. Eval: 0.0002 s/iter. Total: 1.2316 s/iter. ETA=0:00:27\n",
      "\u001B[32m[07/18 01:23:58 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0005 s/iter. Inference: 1.2324 s/iter. Eval: 0.0002 s/iter. Total: 1.2332 s/iter. ETA=0:00:22\n",
      "\u001B[32m[07/18 01:24:03 d2.evaluation.evaluator]: \u001B[0mInference done 65/79. Dataloading: 0.0005 s/iter. Inference: 1.2380 s/iter. Eval: 0.0002 s/iter. Total: 1.2388 s/iter. ETA=0:00:17\n",
      "\u001B[32m[07/18 01:24:08 d2.evaluation.evaluator]: \u001B[0mInference done 69/79. Dataloading: 0.0005 s/iter. Inference: 1.2433 s/iter. Eval: 0.0002 s/iter. Total: 1.2442 s/iter. ETA=0:00:12\n",
      "\u001B[32m[07/18 01:24:14 d2.evaluation.evaluator]: \u001B[0mInference done 73/79. Dataloading: 0.0005 s/iter. Inference: 1.2486 s/iter. Eval: 0.0002 s/iter. Total: 1.2495 s/iter. ETA=0:00:07\n",
      "\u001B[32m[07/18 01:24:19 d2.evaluation.evaluator]: \u001B[0mInference done 77/79. Dataloading: 0.0005 s/iter. Inference: 1.2511 s/iter. Eval: 0.0002 s/iter. Total: 1.2519 s/iter. ETA=0:00:02\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:33.098457 (1.258087 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:32 (1.252069 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.018\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.065\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.020\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.030\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.148\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.148\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 1.816 | 6.488  | 0.220  |  nan  |  nan  | 2.025 |\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.079 | book       | 3.554 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:24:22 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:24:22 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:24:22 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:24:22 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:24:22 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:24:22 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:24:40 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0005 s/iter. Inference: 1.2788 s/iter. Eval: 0.0003 s/iter. Total: 1.2795 s/iter. ETA=0:01:27\n",
      "\u001B[32m[07/18 01:24:46 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0005 s/iter. Inference: 1.2309 s/iter. Eval: 0.0004 s/iter. Total: 1.2319 s/iter. ETA=0:01:17\n",
      "\u001B[32m[07/18 01:24:52 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0005 s/iter. Inference: 1.2308 s/iter. Eval: 0.0003 s/iter. Total: 1.2317 s/iter. ETA=0:01:11\n",
      "\u001B[32m[07/18 01:24:58 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0005 s/iter. Inference: 1.2219 s/iter. Eval: 0.0003 s/iter. Total: 1.2228 s/iter. ETA=0:01:04\n",
      "\u001B[32m[07/18 01:25:04 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.2154 s/iter. Eval: 0.0003 s/iter. Total: 1.2163 s/iter. ETA=0:00:58\n",
      "\u001B[32m[07/18 01:25:10 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0005 s/iter. Inference: 1.2067 s/iter. Eval: 0.0002 s/iter. Total: 1.2076 s/iter. ETA=0:00:51\n",
      "\u001B[32m[07/18 01:25:16 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0005 s/iter. Inference: 1.2021 s/iter. Eval: 0.0002 s/iter. Total: 1.2029 s/iter. ETA=0:00:45\n",
      "\u001B[32m[07/18 01:25:22 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0004 s/iter. Inference: 1.1986 s/iter. Eval: 0.0002 s/iter. Total: 1.1994 s/iter. ETA=0:00:39\n",
      "\u001B[32m[07/18 01:25:27 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0004 s/iter. Inference: 1.1933 s/iter. Eval: 0.0002 s/iter. Total: 1.1941 s/iter. ETA=0:00:33\n",
      "\u001B[32m[07/18 01:25:33 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0004 s/iter. Inference: 1.1916 s/iter. Eval: 0.0002 s/iter. Total: 1.1924 s/iter. ETA=0:00:27\n",
      "\u001B[32m[07/18 01:25:39 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0004 s/iter. Inference: 1.1897 s/iter. Eval: 0.0002 s/iter. Total: 1.1905 s/iter. ETA=0:00:21\n",
      "\u001B[32m[07/18 01:25:45 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0004 s/iter. Inference: 1.1884 s/iter. Eval: 0.0002 s/iter. Total: 1.1892 s/iter. ETA=0:00:15\n",
      "\u001B[32m[07/18 01:25:51 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0005 s/iter. Inference: 1.1890 s/iter. Eval: 0.0002 s/iter. Total: 1.1898 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 01:25:57 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0004 s/iter. Inference: 1.1928 s/iter. Eval: 0.0002 s/iter. Total: 1.1936 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:28.596902 (1.197255 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:28 (1.192074 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.018\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.065\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.020\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.030\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.148\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.148\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 1.816 | 6.488  | 0.220  |  nan  |  nan  | 2.025 |\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:26:01 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.079 | book       | 3.554 |\n",
      "\u001B[32m[07/18 01:26:59 d2.utils.events]: \u001B[0m eta: 0:30:52  iter: 659  total_loss: 1.827  loss_cls: 0.7664  loss_box_reg: 0.8702  loss_rpn_cls: 0.02033  loss_rpn_loc: 0.1727    time: 7.2226  last_time: 5.6023  data_time: 0.0013  last_data_time: 0.0017   lr: 1.7328e-06  \n",
      "\u001B[32m[07/18 01:28:50 d2.utils.events]: \u001B[0m eta: 0:29:07  iter: 679  total_loss: 1.808  loss_cls: 0.7535  loss_box_reg: 0.8664  loss_rpn_cls: 0.0176  loss_rpn_loc: 0.1759    time: 7.1738  last_time: 5.5104  data_time: 0.0012  last_data_time: 0.0022   lr: 1.7778e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:30:42 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:30:42 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:30:42 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:30:42 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:30:42 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:30:42 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:30:59 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0008 s/iter. Inference: 1.2853 s/iter. Eval: 0.0002 s/iter. Total: 1.2863 s/iter. ETA=0:01:27\n",
      "\u001B[32m[07/18 01:31:04 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0006 s/iter. Inference: 1.2985 s/iter. Eval: 0.0002 s/iter. Total: 1.2994 s/iter. ETA=0:01:23\n",
      "\u001B[32m[07/18 01:31:09 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0006 s/iter. Inference: 1.3210 s/iter. Eval: 0.0002 s/iter. Total: 1.3219 s/iter. ETA=0:01:19\n",
      "\u001B[32m[07/18 01:31:15 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0005 s/iter. Inference: 1.3144 s/iter. Eval: 0.0002 s/iter. Total: 1.3153 s/iter. ETA=0:01:13\n",
      "\u001B[32m[07/18 01:31:20 d2.evaluation.evaluator]: \u001B[0mInference done 27/79. Dataloading: 0.0006 s/iter. Inference: 1.3180 s/iter. Eval: 0.0002 s/iter. Total: 1.3190 s/iter. ETA=0:01:08\n",
      "\u001B[32m[07/18 01:31:25 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0006 s/iter. Inference: 1.3222 s/iter. Eval: 0.0002 s/iter. Total: 1.3232 s/iter. ETA=0:01:03\n",
      "\u001B[32m[07/18 01:31:31 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0006 s/iter. Inference: 1.3227 s/iter. Eval: 0.0002 s/iter. Total: 1.3237 s/iter. ETA=0:00:58\n",
      "\u001B[32m[07/18 01:31:37 d2.evaluation.evaluator]: \u001B[0mInference done 40/79. Dataloading: 0.0006 s/iter. Inference: 1.3203 s/iter. Eval: 0.0002 s/iter. Total: 1.3213 s/iter. ETA=0:00:51\n",
      "\u001B[32m[07/18 01:31:43 d2.evaluation.evaluator]: \u001B[0mInference done 44/79. Dataloading: 0.0006 s/iter. Inference: 1.3257 s/iter. Eval: 0.0002 s/iter. Total: 1.3267 s/iter. ETA=0:00:46\n",
      "\u001B[32m[07/18 01:31:48 d2.evaluation.evaluator]: \u001B[0mInference done 48/79. Dataloading: 0.0006 s/iter. Inference: 1.3234 s/iter. Eval: 0.0002 s/iter. Total: 1.3243 s/iter. ETA=0:00:41\n",
      "\u001B[32m[07/18 01:31:54 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0006 s/iter. Inference: 1.3199 s/iter. Eval: 0.0002 s/iter. Total: 1.3208 s/iter. ETA=0:00:34\n",
      "\u001B[32m[07/18 01:32:01 d2.evaluation.evaluator]: \u001B[0mInference done 58/79. Dataloading: 0.0006 s/iter. Inference: 1.3136 s/iter. Eval: 0.0002 s/iter. Total: 1.3146 s/iter. ETA=0:00:27\n",
      "\u001B[32m[07/18 01:32:06 d2.evaluation.evaluator]: \u001B[0mInference done 62/79. Dataloading: 0.0006 s/iter. Inference: 1.3096 s/iter. Eval: 0.0002 s/iter. Total: 1.3106 s/iter. ETA=0:00:22\n",
      "\u001B[32m[07/18 01:32:12 d2.evaluation.evaluator]: \u001B[0mInference done 67/79. Dataloading: 0.0006 s/iter. Inference: 1.3061 s/iter. Eval: 0.0002 s/iter. Total: 1.3071 s/iter. ETA=0:00:15\n",
      "\u001B[32m[07/18 01:32:17 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0005 s/iter. Inference: 1.3085 s/iter. Eval: 0.0002 s/iter. Total: 1.3094 s/iter. ETA=0:00:10\n",
      "\u001B[32m[07/18 01:32:24 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0005 s/iter. Inference: 1.3044 s/iter. Eval: 0.0002 s/iter. Total: 1.3053 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:36.727402 (1.307127 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:36 (1.300590 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.023\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.017\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.158\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 2.074 | 7.220  | 0.328  |  nan  |  nan  | 2.296 |\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.095 | book       | 4.053 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:32:28 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:32:28 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:32:28 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:32:28 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:32:28 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:32:28 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:32:45 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0009 s/iter. Inference: 1.2418 s/iter. Eval: 0.0002 s/iter. Total: 1.2429 s/iter. ETA=0:01:24\n",
      "\u001B[32m[07/18 01:32:50 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0007 s/iter. Inference: 1.2468 s/iter. Eval: 0.0002 s/iter. Total: 1.2478 s/iter. ETA=0:01:19\n",
      "\u001B[32m[07/18 01:32:56 d2.evaluation.evaluator]: \u001B[0mInference done 20/79. Dataloading: 0.0006 s/iter. Inference: 1.2344 s/iter. Eval: 0.0002 s/iter. Total: 1.2354 s/iter. ETA=0:01:12\n",
      "\u001B[32m[07/18 01:33:01 d2.evaluation.evaluator]: \u001B[0mInference done 24/79. Dataloading: 0.0008 s/iter. Inference: 1.2459 s/iter. Eval: 0.0002 s/iter. Total: 1.2470 s/iter. ETA=0:01:08\n",
      "\u001B[32m[07/18 01:33:06 d2.evaluation.evaluator]: \u001B[0mInference done 28/79. Dataloading: 0.0007 s/iter. Inference: 1.2532 s/iter. Eval: 0.0002 s/iter. Total: 1.2543 s/iter. ETA=0:01:03\n",
      "\u001B[32m[07/18 01:33:12 d2.evaluation.evaluator]: \u001B[0mInference done 32/79. Dataloading: 0.0007 s/iter. Inference: 1.2631 s/iter. Eval: 0.0002 s/iter. Total: 1.2641 s/iter. ETA=0:00:59\n",
      "\u001B[32m[07/18 01:33:17 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0007 s/iter. Inference: 1.2651 s/iter. Eval: 0.0002 s/iter. Total: 1.2661 s/iter. ETA=0:00:54\n",
      "\u001B[32m[07/18 01:33:22 d2.evaluation.evaluator]: \u001B[0mInference done 40/79. Dataloading: 0.0006 s/iter. Inference: 1.2653 s/iter. Eval: 0.0002 s/iter. Total: 1.2663 s/iter. ETA=0:00:49\n",
      "\u001B[32m[07/18 01:33:27 d2.evaluation.evaluator]: \u001B[0mInference done 44/79. Dataloading: 0.0006 s/iter. Inference: 1.2665 s/iter. Eval: 0.0002 s/iter. Total: 1.2674 s/iter. ETA=0:00:44\n",
      "\u001B[32m[07/18 01:33:32 d2.evaluation.evaluator]: \u001B[0mInference done 48/79. Dataloading: 0.0006 s/iter. Inference: 1.2728 s/iter. Eval: 0.0003 s/iter. Total: 1.2738 s/iter. ETA=0:00:39\n",
      "\u001B[32m[07/18 01:33:38 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0006 s/iter. Inference: 1.2666 s/iter. Eval: 0.0002 s/iter. Total: 1.2676 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 01:33:44 d2.evaluation.evaluator]: \u001B[0mInference done 57/79. Dataloading: 0.0006 s/iter. Inference: 1.2682 s/iter. Eval: 0.0002 s/iter. Total: 1.2692 s/iter. ETA=0:00:27\n",
      "\u001B[32m[07/18 01:33:49 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0006 s/iter. Inference: 1.2702 s/iter. Eval: 0.0002 s/iter. Total: 1.2711 s/iter. ETA=0:00:22\n",
      "\u001B[32m[07/18 01:33:55 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0006 s/iter. Inference: 1.2655 s/iter. Eval: 0.0002 s/iter. Total: 1.2665 s/iter. ETA=0:00:16\n",
      "\u001B[32m[07/18 01:34:00 d2.evaluation.evaluator]: \u001B[0mInference done 70/79. Dataloading: 0.0006 s/iter. Inference: 1.2686 s/iter. Eval: 0.0002 s/iter. Total: 1.2695 s/iter. ETA=0:00:11\n",
      "\u001B[32m[07/18 01:34:06 d2.evaluation.evaluator]: \u001B[0mInference done 75/79. Dataloading: 0.0006 s/iter. Inference: 1.2651 s/iter. Eval: 0.0002 s/iter. Total: 1.2660 s/iter. ETA=0:00:05\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:34.037187 (1.270773 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:33 (1.264101 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.023\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.017\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.158\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 2.074 | 7.220  | 0.328  |  nan  |  nan  | 2.296 |\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:34:12 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.095 | book       | 4.053 |\n",
      "\u001B[32m[07/18 01:34:12 d2.utils.events]: \u001B[0m eta: 0:27:21  iter: 699  total_loss: 1.847  loss_cls: 0.7467  loss_box_reg: 0.8901  loss_rpn_cls: 0.02489  loss_rpn_loc: 0.1822    time: 7.2800  last_time: 111.8732  data_time: 0.0011  last_data_time: 0.0012   lr: 1.8228e-06  \n",
      "\u001B[32m[07/18 01:36:04 d2.utils.events]: \u001B[0m eta: 0:25:33  iter: 719  total_loss: 1.789  loss_cls: 0.7415  loss_box_reg: 0.8597  loss_rpn_cls: 0.01983  loss_rpn_loc: 0.1646    time: 7.2338  last_time: 4.5343  data_time: 0.0012  last_data_time: 0.0012   lr: 1.8678e-06  \n",
      "\u001B[32m[07/18 01:37:53 d2.utils.events]: \u001B[0m eta: 0:23:44  iter: 739  total_loss: 1.785  loss_cls: 0.7346  loss_box_reg: 0.8522  loss_rpn_cls: 0.01962  loss_rpn_loc: 0.1733    time: 7.1858  last_time: 5.3675  data_time: 0.0012  last_data_time: 0.0010   lr: 1.9128e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:38:47 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:38:47 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:38:47 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:38:47 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:38:47 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:38:47 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:39:02 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.1252 s/iter. Eval: 0.0002 s/iter. Total: 1.1259 s/iter. ETA=0:01:16\n",
      "\u001B[32m[07/18 01:39:08 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0004 s/iter. Inference: 1.1356 s/iter. Eval: 0.0002 s/iter. Total: 1.1362 s/iter. ETA=0:01:11\n",
      "\u001B[32m[07/18 01:39:14 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0004 s/iter. Inference: 1.1448 s/iter. Eval: 0.0002 s/iter. Total: 1.1455 s/iter. ETA=0:01:06\n",
      "\u001B[32m[07/18 01:39:20 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0005 s/iter. Inference: 1.1463 s/iter. Eval: 0.0002 s/iter. Total: 1.1472 s/iter. ETA=0:01:00\n",
      "\u001B[32m[07/18 01:39:25 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.1535 s/iter. Eval: 0.0002 s/iter. Total: 1.1543 s/iter. ETA=0:00:55\n",
      "\u001B[32m[07/18 01:39:31 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0005 s/iter. Inference: 1.1553 s/iter. Eval: 0.0002 s/iter. Total: 1.1562 s/iter. ETA=0:00:49\n",
      "\u001B[32m[07/18 01:39:37 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0005 s/iter. Inference: 1.1591 s/iter. Eval: 0.0002 s/iter. Total: 1.1599 s/iter. ETA=0:00:44\n",
      "\u001B[32m[07/18 01:39:43 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0005 s/iter. Inference: 1.1613 s/iter. Eval: 0.0002 s/iter. Total: 1.1621 s/iter. ETA=0:00:38\n",
      "\u001B[32m[07/18 01:39:49 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0005 s/iter. Inference: 1.1598 s/iter. Eval: 0.0002 s/iter. Total: 1.1606 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 01:39:55 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0005 s/iter. Inference: 1.1593 s/iter. Eval: 0.0002 s/iter. Total: 1.1601 s/iter. ETA=0:00:26\n",
      "\u001B[32m[07/18 01:40:00 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0005 s/iter. Inference: 1.1582 s/iter. Eval: 0.0002 s/iter. Total: 1.1590 s/iter. ETA=0:00:20\n",
      "\u001B[32m[07/18 01:40:06 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0005 s/iter. Inference: 1.1566 s/iter. Eval: 0.0002 s/iter. Total: 1.1574 s/iter. ETA=0:00:15\n",
      "\u001B[32m[07/18 01:40:12 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0005 s/iter. Inference: 1.1561 s/iter. Eval: 0.0002 s/iter. Total: 1.1569 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 01:40:18 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0005 s/iter. Inference: 1.1576 s/iter. Eval: 0.0002 s/iter. Total: 1.1584 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:26.185853 (1.164674 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:25 (1.159346 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.081\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.027\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.017\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.036\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.172\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.172\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 2.428 | 8.110  | 0.454  |  nan  |  nan  | 2.686 |\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.111 | book       | 4.745 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:40:22 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:40:22 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:40:22 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:40:22 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:40:22 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:40:22 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:40:38 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.2688 s/iter. Eval: 0.0002 s/iter. Total: 1.2693 s/iter. ETA=0:01:26\n",
      "\u001B[32m[07/18 01:40:43 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0004 s/iter. Inference: 1.2197 s/iter. Eval: 0.0002 s/iter. Total: 1.2204 s/iter. ETA=0:01:16\n",
      "\u001B[32m[07/18 01:40:49 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0004 s/iter. Inference: 1.2029 s/iter. Eval: 0.0002 s/iter. Total: 1.2036 s/iter. ETA=0:01:09\n",
      "\u001B[32m[07/18 01:40:55 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0004 s/iter. Inference: 1.1996 s/iter. Eval: 0.0001 s/iter. Total: 1.2003 s/iter. ETA=0:01:03\n",
      "\u001B[32m[07/18 01:41:01 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0004 s/iter. Inference: 1.1964 s/iter. Eval: 0.0002 s/iter. Total: 1.1971 s/iter. ETA=0:00:57\n",
      "\u001B[32m[07/18 01:41:07 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0004 s/iter. Inference: 1.1935 s/iter. Eval: 0.0002 s/iter. Total: 1.1942 s/iter. ETA=0:00:51\n",
      "\u001B[32m[07/18 01:41:13 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 1.1918 s/iter. Eval: 0.0001 s/iter. Total: 1.1925 s/iter. ETA=0:00:45\n",
      "\u001B[32m[07/18 01:41:19 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0004 s/iter. Inference: 1.1864 s/iter. Eval: 0.0001 s/iter. Total: 1.1872 s/iter. ETA=0:00:39\n",
      "\u001B[32m[07/18 01:41:24 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0004 s/iter. Inference: 1.1810 s/iter. Eval: 0.0001 s/iter. Total: 1.1818 s/iter. ETA=0:00:33\n",
      "\u001B[32m[07/18 01:41:30 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0004 s/iter. Inference: 1.1791 s/iter. Eval: 0.0001 s/iter. Total: 1.1798 s/iter. ETA=0:00:27\n",
      "\u001B[32m[07/18 01:41:36 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0005 s/iter. Inference: 1.1750 s/iter. Eval: 0.0001 s/iter. Total: 1.1757 s/iter. ETA=0:00:21\n",
      "\u001B[32m[07/18 01:41:42 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0005 s/iter. Inference: 1.1727 s/iter. Eval: 0.0001 s/iter. Total: 1.1735 s/iter. ETA=0:00:15\n",
      "\u001B[32m[07/18 01:41:47 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0005 s/iter. Inference: 1.1712 s/iter. Eval: 0.0001 s/iter. Total: 1.1720 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 01:41:53 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0005 s/iter. Inference: 1.1696 s/iter. Eval: 0.0001 s/iter. Total: 1.1704 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:26.963761 (1.175186 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:26 (1.169271 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.08 seconds.\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.081\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.027\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.017\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.036\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.172\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.172\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 2.428 | 8.110  | 0.454  |  nan  |  nan  | 2.686 |\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:41:57 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.111 | book       | 4.745 |\n",
      "\u001B[32m[07/18 01:42:50 d2.utils.events]: \u001B[0m eta: 0:21:54  iter: 759  total_loss: 1.798  loss_cls: 0.7245  loss_box_reg: 0.8742  loss_rpn_cls: 0.02032  loss_rpn_loc: 0.1665    time: 7.2611  last_time: 4.3335  data_time: 0.0012  last_data_time: 0.0013   lr: 1.9578e-06  \n",
      "\u001B[32m[07/18 01:44:35 d2.utils.events]: \u001B[0m eta: 0:20:01  iter: 779  total_loss: 1.749  loss_cls: 0.7154  loss_box_reg: 0.8375  loss_rpn_cls: 0.02067  loss_rpn_loc: 0.1722    time: 7.2093  last_time: 5.1172  data_time: 0.0013  last_data_time: 0.0010   lr: 2.0028e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:46:18 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:46:18 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:46:18 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:46:18 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:46:18 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:46:18 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:46:34 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0005 s/iter. Inference: 1.1843 s/iter. Eval: 0.0002 s/iter. Total: 1.1849 s/iter. ETA=0:01:20\n",
      "\u001B[32m[07/18 01:46:39 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0005 s/iter. Inference: 1.1614 s/iter. Eval: 0.0002 s/iter. Total: 1.1622 s/iter. ETA=0:01:13\n",
      "\u001B[32m[07/18 01:46:45 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0005 s/iter. Inference: 1.1574 s/iter. Eval: 0.0002 s/iter. Total: 1.1581 s/iter. ETA=0:01:07\n",
      "\u001B[32m[07/18 01:46:51 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0005 s/iter. Inference: 1.1594 s/iter. Eval: 0.0002 s/iter. Total: 1.1602 s/iter. ETA=0:01:01\n",
      "\u001B[32m[07/18 01:46:57 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.1583 s/iter. Eval: 0.0002 s/iter. Total: 1.1591 s/iter. ETA=0:00:55\n",
      "\u001B[32m[07/18 01:47:02 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0005 s/iter. Inference: 1.1523 s/iter. Eval: 0.0002 s/iter. Total: 1.1531 s/iter. ETA=0:00:49\n",
      "\u001B[32m[07/18 01:47:08 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0005 s/iter. Inference: 1.1522 s/iter. Eval: 0.0002 s/iter. Total: 1.1529 s/iter. ETA=0:00:43\n",
      "\u001B[32m[07/18 01:47:14 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0005 s/iter. Inference: 1.1498 s/iter. Eval: 0.0002 s/iter. Total: 1.1506 s/iter. ETA=0:00:37\n",
      "\u001B[32m[07/18 01:47:19 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0005 s/iter. Inference: 1.1490 s/iter. Eval: 0.0002 s/iter. Total: 1.1498 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 01:47:25 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0005 s/iter. Inference: 1.1476 s/iter. Eval: 0.0002 s/iter. Total: 1.1484 s/iter. ETA=0:00:26\n",
      "\u001B[32m[07/18 01:47:31 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0005 s/iter. Inference: 1.1469 s/iter. Eval: 0.0002 s/iter. Total: 1.1477 s/iter. ETA=0:00:20\n",
      "\u001B[32m[07/18 01:47:36 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0005 s/iter. Inference: 1.1462 s/iter. Eval: 0.0002 s/iter. Total: 1.1470 s/iter. ETA=0:00:14\n",
      "\u001B[32m[07/18 01:47:42 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0005 s/iter. Inference: 1.1454 s/iter. Eval: 0.0002 s/iter. Total: 1.1463 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 01:47:48 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0005 s/iter. Inference: 1.1461 s/iter. Eval: 0.0002 s/iter. Total: 1.1469 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:25.150336 (1.150680 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:24 (1.144134 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.092\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.018\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.186\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.186\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 2.865 | 9.232  | 0.627  |  nan  |  nan  | 3.171 |\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.122 | book       | 5.609 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:47:52 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:47:52 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:47:52 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:47:52 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:47:52 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:47:52 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:48:07 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.1439 s/iter. Eval: 0.0001 s/iter. Total: 1.1444 s/iter. ETA=0:01:17\n",
      "\u001B[32m[07/18 01:48:13 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0006 s/iter. Inference: 1.1690 s/iter. Eval: 0.0001 s/iter. Total: 1.1701 s/iter. ETA=0:01:13\n",
      "\u001B[32m[07/18 01:48:19 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0006 s/iter. Inference: 1.1671 s/iter. Eval: 0.0001 s/iter. Total: 1.1680 s/iter. ETA=0:01:07\n",
      "\u001B[32m[07/18 01:48:24 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0005 s/iter. Inference: 1.1577 s/iter. Eval: 0.0001 s/iter. Total: 1.1586 s/iter. ETA=0:01:01\n",
      "\u001B[32m[07/18 01:48:30 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0005 s/iter. Inference: 1.1477 s/iter. Eval: 0.0002 s/iter. Total: 1.1485 s/iter. ETA=0:00:55\n",
      "\u001B[32m[07/18 01:48:36 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0005 s/iter. Inference: 1.1506 s/iter. Eval: 0.0001 s/iter. Total: 1.1515 s/iter. ETA=0:00:49\n",
      "\u001B[32m[07/18 01:48:42 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0005 s/iter. Inference: 1.1518 s/iter. Eval: 0.0001 s/iter. Total: 1.1527 s/iter. ETA=0:00:43\n",
      "\u001B[32m[07/18 01:48:48 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0005 s/iter. Inference: 1.1549 s/iter. Eval: 0.0001 s/iter. Total: 1.1558 s/iter. ETA=0:00:38\n",
      "\u001B[32m[07/18 01:48:53 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0005 s/iter. Inference: 1.1514 s/iter. Eval: 0.0001 s/iter. Total: 1.1522 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 01:48:59 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0005 s/iter. Inference: 1.1512 s/iter. Eval: 0.0001 s/iter. Total: 1.1520 s/iter. ETA=0:00:26\n",
      "\u001B[32m[07/18 01:49:05 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0005 s/iter. Inference: 1.1533 s/iter. Eval: 0.0002 s/iter. Total: 1.1541 s/iter. ETA=0:00:20\n",
      "\u001B[32m[07/18 01:49:10 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0005 s/iter. Inference: 1.1525 s/iter. Eval: 0.0002 s/iter. Total: 1.1533 s/iter. ETA=0:00:14\n",
      "\u001B[32m[07/18 01:49:16 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0005 s/iter. Inference: 1.1519 s/iter. Eval: 0.0002 s/iter. Total: 1.1528 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 01:49:22 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0005 s/iter. Inference: 1.1509 s/iter. Eval: 0.0002 s/iter. Total: 1.1517 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:25.532105 (1.155839 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:25 (1.150956 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.092\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.018\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.186\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.186\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 2.865 | 9.232  | 0.627  |  nan  |  nan  | 3.171 |\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 01:49:26 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.122 | book       | 5.609 |\n",
      "\u001B[32m[07/18 01:49:26 d2.utils.events]: \u001B[0m eta: 0:18:10  iter: 799  total_loss: 1.76  loss_cls: 0.7116  loss_box_reg: 0.8637  loss_rpn_cls: 0.02277  loss_rpn_loc: 0.1734    time: 7.2755  last_time: 98.2170  data_time: 0.0012  last_data_time: 0.0008   lr: 2.0478e-06  \n",
      "\u001B[32m[07/18 01:51:16 d2.utils.events]: \u001B[0m eta: 0:16:22  iter: 819  total_loss: 1.753  loss_cls: 0.7069  loss_box_reg: 0.8585  loss_rpn_cls: 0.01954  loss_rpn_loc: 0.1685    time: 7.2326  last_time: 5.9178  data_time: 0.0012  last_data_time: 0.0010   lr: 2.0928e-06  \n",
      "\u001B[32m[07/18 01:53:05 d2.utils.events]: \u001B[0m eta: 0:14:33  iter: 839  total_loss: 1.759  loss_cls: 0.7008  loss_box_reg: 0.8591  loss_rpn_cls: 0.01802  loss_rpn_loc: 0.1702    time: 7.1896  last_time: 4.9366  data_time: 0.0012  last_data_time: 0.0023   lr: 2.1378e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 01:53:59 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 01:53:59 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 01:53:59 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 01:53:59 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 01:53:59 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 01:53:59 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 01:54:16 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0004 s/iter. Inference: 1.3042 s/iter. Eval: 0.0002 s/iter. Total: 1.3048 s/iter. ETA=0:01:28\n",
      "\u001B[32m[07/18 01:54:22 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0004 s/iter. Inference: 1.2775 s/iter. Eval: 0.0002 s/iter. Total: 1.2783 s/iter. ETA=0:01:20\n",
      "\u001B[32m[07/18 01:54:28 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0004 s/iter. Inference: 1.2521 s/iter. Eval: 0.0002 s/iter. Total: 1.2529 s/iter. ETA=0:01:12\n",
      "\u001B[32m[07/18 01:54:34 d2.evaluation.evaluator]: \u001B[0mInference done 25/79. Dataloading: 0.0004 s/iter. Inference: 1.2787 s/iter. Eval: 0.0002 s/iter. Total: 1.2794 s/iter. ETA=0:01:09\n",
      "\u001B[32m[07/18 01:54:39 d2.evaluation.evaluator]: \u001B[0mInference done 29/79. Dataloading: 0.0004 s/iter. Inference: 1.2905 s/iter. Eval: 0.0002 s/iter. Total: 1.2913 s/iter. ETA=0:01:04\n",
      "\u001B[32m[07/18 02:04:45 d2.evaluation.evaluator]: \u001B[0mInference done 32/79. Dataloading: 0.0004 s/iter. Inference: 1.3016 s/iter. Eval: 0.0002 s/iter. Total: 1.3024 s/iter. ETA=0:01:01\n",
      "\u001B[32m[07/18 02:04:50 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0004 s/iter. Inference: 1.3221 s/iter. Eval: 0.0002 s/iter. Total: 1.3228 s/iter. ETA=0:00:56\n",
      "\u001B[32m[07/18 02:04:57 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 1.3147 s/iter. Eval: 0.0002 s/iter. Total: 1.3155 s/iter. ETA=0:00:49\n",
      "\u001B[32m[07/18 02:05:03 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0004 s/iter. Inference: 1.3066 s/iter. Eval: 0.0002 s/iter. Total: 1.3074 s/iter. ETA=0:00:43\n",
      "\u001B[32m[07/18 02:05:09 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0005 s/iter. Inference: 1.2994 s/iter. Eval: 0.0002 s/iter. Total: 1.3002 s/iter. ETA=0:00:36\n",
      "\u001B[32m[07/18 02:05:14 d2.evaluation.evaluator]: \u001B[0mInference done 55/79. Dataloading: 0.0004 s/iter. Inference: 1.2986 s/iter. Eval: 0.0002 s/iter. Total: 1.2994 s/iter. ETA=0:00:31\n",
      "\u001B[32m[07/18 02:05:21 d2.evaluation.evaluator]: \u001B[0mInference done 60/79. Dataloading: 0.0004 s/iter. Inference: 1.3002 s/iter. Eval: 0.0002 s/iter. Total: 1.3010 s/iter. ETA=0:00:24\n",
      "\u001B[32m[07/18 02:05:27 d2.evaluation.evaluator]: \u001B[0mInference done 65/79. Dataloading: 0.0005 s/iter. Inference: 1.3000 s/iter. Eval: 0.0002 s/iter. Total: 1.3008 s/iter. ETA=0:00:18\n",
      "\u001B[32m[07/18 02:20:50 d2.evaluation.evaluator]: \u001B[0mInference done 68/79. Dataloading: 0.0005 s/iter. Inference: 1.3059 s/iter. Eval: 0.0002 s/iter. Total: 1.3067 s/iter. ETA=0:00:14\n",
      "\u001B[32m[07/18 02:20:56 d2.evaluation.evaluator]: \u001B[0mInference done 72/79. Dataloading: 0.0005 s/iter. Inference: 1.3213 s/iter. Eval: 0.0002 s/iter. Total: 1.3221 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 02:21:01 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0005 s/iter. Inference: 1.3225 s/iter. Eval: 0.0002 s/iter. Total: 1.3233 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:38.700016 (1.333784 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:38 (1.328166 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.106\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.018\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.202\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.202\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 3.438 | 10.648 | 0.865  |  nan  |  nan  | 3.772 |\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.127 | book       | 6.750 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 02:21:06 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 02:21:06 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 02:21:06 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 02:21:06 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 02:21:06 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 02:21:06 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 02:21:24 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.3267 s/iter. Eval: 0.0002 s/iter. Total: 1.3272 s/iter. ETA=0:01:30\n",
      "\u001B[32m[07/18 02:21:29 d2.evaluation.evaluator]: \u001B[0mInference done 15/79. Dataloading: 0.0004 s/iter. Inference: 1.3367 s/iter. Eval: 0.0002 s/iter. Total: 1.3374 s/iter. ETA=0:01:25\n",
      "\u001B[32m[07/18 02:21:34 d2.evaluation.evaluator]: \u001B[0mInference done 19/79. Dataloading: 0.0004 s/iter. Inference: 1.3144 s/iter. Eval: 0.0002 s/iter. Total: 1.3151 s/iter. ETA=0:01:18\n",
      "\u001B[32m[07/18 02:37:12 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0004 s/iter. Inference: 1.3264 s/iter. Eval: 0.0002 s/iter. Total: 1.3271 s/iter. ETA=0:01:16\n",
      "\u001B[32m[07/18 02:37:18 d2.evaluation.evaluator]: \u001B[0mInference done 25/79. Dataloading: 0.0004 s/iter. Inference: 1.3517 s/iter. Eval: 0.0002 s/iter. Total: 1.3524 s/iter. ETA=0:01:13\n",
      "\u001B[32m[07/18 02:37:23 d2.evaluation.evaluator]: \u001B[0mInference done 29/79. Dataloading: 0.0004 s/iter. Inference: 1.3467 s/iter. Eval: 0.0002 s/iter. Total: 1.3475 s/iter. ETA=0:01:07\n",
      "\u001B[32m[07/18 02:37:29 d2.evaluation.evaluator]: \u001B[0mInference done 33/79. Dataloading: 0.0004 s/iter. Inference: 1.3610 s/iter. Eval: 0.0002 s/iter. Total: 1.3618 s/iter. ETA=0:01:02\n",
      "\u001B[32m[07/18 02:37:34 d2.evaluation.evaluator]: \u001B[0mInference done 37/79. Dataloading: 0.0004 s/iter. Inference: 1.3732 s/iter. Eval: 0.0002 s/iter. Total: 1.3740 s/iter. ETA=0:00:57\n",
      "\u001B[32m[07/18 02:37:40 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 1.3624 s/iter. Eval: 0.0002 s/iter. Total: 1.3632 s/iter. ETA=0:00:51\n",
      "\u001B[32m[07/18 02:37:45 d2.evaluation.evaluator]: \u001B[0mInference done 45/79. Dataloading: 0.0004 s/iter. Inference: 1.3545 s/iter. Eval: 0.0002 s/iter. Total: 1.3553 s/iter. ETA=0:00:46\n",
      "\u001B[32m[07/18 02:37:50 d2.evaluation.evaluator]: \u001B[0mInference done 49/79. Dataloading: 0.0004 s/iter. Inference: 1.3530 s/iter. Eval: 0.0002 s/iter. Total: 1.3538 s/iter. ETA=0:00:40\n",
      "\u001B[32m[07/18 02:37:57 d2.evaluation.evaluator]: \u001B[0mInference done 54/79. Dataloading: 0.0004 s/iter. Inference: 1.3483 s/iter. Eval: 0.0002 s/iter. Total: 1.3491 s/iter. ETA=0:00:33\n",
      "\u001B[32m[07/18 02:38:03 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0004 s/iter. Inference: 1.3381 s/iter. Eval: 0.0001 s/iter. Total: 1.3389 s/iter. ETA=0:00:26\n",
      "\u001B[32m[07/18 02:38:09 d2.evaluation.evaluator]: \u001B[0mInference done 64/79. Dataloading: 0.0004 s/iter. Inference: 1.3330 s/iter. Eval: 0.0001 s/iter. Total: 1.3337 s/iter. ETA=0:00:20\n",
      "\u001B[32m[07/18 02:49:42 d2.evaluation.evaluator]: \u001B[0mInference done 68/79. Dataloading: 0.0004 s/iter. Inference: 1.3320 s/iter. Eval: 0.0001 s/iter. Total: 1.3328 s/iter. ETA=0:00:14\n",
      "\u001B[32m[07/18 02:49:48 d2.evaluation.evaluator]: \u001B[0mInference done 72/79. Dataloading: 0.0004 s/iter. Inference: 1.3441 s/iter. Eval: 0.0001 s/iter. Total: 1.3449 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 02:49:54 d2.evaluation.evaluator]: \u001B[0mInference done 77/79. Dataloading: 0.0004 s/iter. Inference: 1.3372 s/iter. Eval: 0.0001 s/iter. Total: 1.3380 s/iter. ETA=0:00:02\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:39.146546 (1.339818 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:38 (1.333859 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.106\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.018\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.202\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.202\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 3.438 | 10.648 | 0.865  |  nan  |  nan  | 3.772 |\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 02:49:57 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.127 | book       | 6.750 |\n",
      "\u001B[32m[07/18 03:07:21 d2.utils.events]: \u001B[0m eta: 0:12:44  iter: 859  total_loss: 1.767  loss_cls: 0.6938  loss_box_reg: 0.8704  loss_rpn_cls: 0.02048  loss_rpn_loc: 0.1711    time: 7.2767  last_time: 5.6626  data_time: 0.0013  last_data_time: 0.0008   lr: 2.1828e-06  \n",
      "\u001B[32m[07/18 03:24:46 d2.utils.events]: \u001B[0m eta: 0:10:56  iter: 879  total_loss: 1.732  loss_cls: 0.684  loss_box_reg: 0.8524  loss_rpn_cls: 0.01907  loss_rpn_loc: 0.1644    time: 7.2362  last_time: 5.6991  data_time: 0.0041  last_data_time: 0.0012   lr: 2.2277e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 03:38:56 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 03:38:56 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 03:38:56 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 03:38:56 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 03:38:56 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 03:38:56 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 03:39:12 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.1701 s/iter. Eval: 0.0002 s/iter. Total: 1.1706 s/iter. ETA=0:01:19\n",
      "\u001B[32m[07/18 03:39:17 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0004 s/iter. Inference: 1.1656 s/iter. Eval: 0.0002 s/iter. Total: 1.1664 s/iter. ETA=0:01:13\n",
      "\u001B[32m[07/18 03:39:23 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0004 s/iter. Inference: 1.1618 s/iter. Eval: 0.0002 s/iter. Total: 1.1626 s/iter. ETA=0:01:07\n",
      "\u001B[32m[07/18 03:39:29 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0004 s/iter. Inference: 1.1645 s/iter. Eval: 0.0002 s/iter. Total: 1.1652 s/iter. ETA=0:01:01\n",
      "\u001B[32m[07/18 03:39:35 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0004 s/iter. Inference: 1.1696 s/iter. Eval: 0.0002 s/iter. Total: 1.1703 s/iter. ETA=0:00:56\n",
      "\u001B[32m[07/18 03:39:41 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0004 s/iter. Inference: 1.1615 s/iter. Eval: 0.0002 s/iter. Total: 1.1622 s/iter. ETA=0:00:49\n",
      "\u001B[32m[07/18 03:39:46 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 1.1621 s/iter. Eval: 0.0002 s/iter. Total: 1.1628 s/iter. ETA=0:00:44\n",
      "\u001B[32m[07/18 03:39:52 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0004 s/iter. Inference: 1.1581 s/iter. Eval: 0.0002 s/iter. Total: 1.1589 s/iter. ETA=0:00:38\n",
      "\u001B[32m[07/18 03:39:58 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0004 s/iter. Inference: 1.1556 s/iter. Eval: 0.0002 s/iter. Total: 1.1564 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 03:40:04 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0004 s/iter. Inference: 1.1538 s/iter. Eval: 0.0002 s/iter. Total: 1.1545 s/iter. ETA=0:00:26\n",
      "\u001B[32m[07/18 03:40:09 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0004 s/iter. Inference: 1.1532 s/iter. Eval: 0.0002 s/iter. Total: 1.1540 s/iter. ETA=0:00:20\n",
      "\u001B[32m[07/18 03:40:15 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0004 s/iter. Inference: 1.1503 s/iter. Eval: 0.0002 s/iter. Total: 1.1510 s/iter. ETA=0:00:14\n",
      "\u001B[32m[07/18 03:40:21 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0004 s/iter. Inference: 1.1517 s/iter. Eval: 0.0002 s/iter. Total: 1.1524 s/iter. ETA=0:00:09\n",
      "\u001B[32m[07/18 03:40:26 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0004 s/iter. Inference: 1.1509 s/iter. Eval: 0.0002 s/iter. Total: 1.1517 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:25.640258 (1.157301 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:25 (1.151200 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.041\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.123\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.045\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.018\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.216\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.216\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 4.103 | 12.269 | 1.169  |  nan  |  nan  | 4.490 |\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.132 | book       | 8.074 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 03:40:30 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 03:40:30 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 03:40:30 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 03:40:30 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 03:40:30 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 03:40:30 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 03:40:46 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0009 s/iter. Inference: 1.3354 s/iter. Eval: 0.0002 s/iter. Total: 1.3365 s/iter. ETA=0:01:30\n",
      "\u001B[32m[07/18 03:50:55 d2.evaluation.evaluator]: \u001B[0mInference done 13/79. Dataloading: 0.0008 s/iter. Inference: 1.4114 s/iter. Eval: 0.0002 s/iter. Total: 1.4126 s/iter. ETA=0:01:33\n",
      "\u001B[32m[07/18 03:51:01 d2.evaluation.evaluator]: \u001B[0mInference done 18/79. Dataloading: 0.0007 s/iter. Inference: 1.3135 s/iter. Eval: 0.0002 s/iter. Total: 1.3145 s/iter. ETA=0:01:20\n",
      "\u001B[32m[07/18 03:51:07 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0006 s/iter. Inference: 1.2781 s/iter. Eval: 0.0001 s/iter. Total: 1.2790 s/iter. ETA=0:01:11\n",
      "\u001B[32m[07/18 03:51:13 d2.evaluation.evaluator]: \u001B[0mInference done 28/79. Dataloading: 0.0006 s/iter. Inference: 1.2530 s/iter. Eval: 0.0001 s/iter. Total: 1.2538 s/iter. ETA=0:01:03\n",
      "\u001B[32m[07/18 03:51:19 d2.evaluation.evaluator]: \u001B[0mInference done 33/79. Dataloading: 0.0006 s/iter. Inference: 1.2362 s/iter. Eval: 0.0001 s/iter. Total: 1.2371 s/iter. ETA=0:00:56\n",
      "\u001B[32m[07/18 03:51:25 d2.evaluation.evaluator]: \u001B[0mInference done 38/79. Dataloading: 0.0006 s/iter. Inference: 1.2262 s/iter. Eval: 0.0001 s/iter. Total: 1.2270 s/iter. ETA=0:00:50\n",
      "\u001B[32m[07/18 03:51:30 d2.evaluation.evaluator]: \u001B[0mInference done 43/79. Dataloading: 0.0005 s/iter. Inference: 1.2169 s/iter. Eval: 0.0001 s/iter. Total: 1.2177 s/iter. ETA=0:00:43\n",
      "\u001B[32m[07/18 03:51:36 d2.evaluation.evaluator]: \u001B[0mInference done 48/79. Dataloading: 0.0005 s/iter. Inference: 1.2113 s/iter. Eval: 0.0001 s/iter. Total: 1.2121 s/iter. ETA=0:00:37\n",
      "\u001B[32m[07/18 03:51:42 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0005 s/iter. Inference: 1.2101 s/iter. Eval: 0.0001 s/iter. Total: 1.2109 s/iter. ETA=0:00:31\n",
      "\u001B[32m[07/18 03:51:48 d2.evaluation.evaluator]: \u001B[0mInference done 58/79. Dataloading: 0.0005 s/iter. Inference: 1.2025 s/iter. Eval: 0.0001 s/iter. Total: 1.2033 s/iter. ETA=0:00:25\n",
      "\u001B[32m[07/18 03:51:54 d2.evaluation.evaluator]: \u001B[0mInference done 63/79. Dataloading: 0.0005 s/iter. Inference: 1.1955 s/iter. Eval: 0.0001 s/iter. Total: 1.1963 s/iter. ETA=0:00:19\n",
      "\u001B[32m[07/18 03:51:59 d2.evaluation.evaluator]: \u001B[0mInference done 68/79. Dataloading: 0.0005 s/iter. Inference: 1.1938 s/iter. Eval: 0.0001 s/iter. Total: 1.1946 s/iter. ETA=0:00:13\n",
      "\u001B[32m[07/18 03:52:05 d2.evaluation.evaluator]: \u001B[0mInference done 73/79. Dataloading: 0.0005 s/iter. Inference: 1.1912 s/iter. Eval: 0.0001 s/iter. Total: 1.1920 s/iter. ETA=0:00:07\n",
      "\u001B[32m[07/18 03:52:11 d2.evaluation.evaluator]: \u001B[0mInference done 78/79. Dataloading: 0.0005 s/iter. Inference: 1.1885 s/iter. Eval: 0.0001 s/iter. Total: 1.1893 s/iter. ETA=0:00:01\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:28.127736 (1.190915 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:27 (1.185687 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.041\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.123\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.045\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.018\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.216\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.216\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 4.103 | 12.269 | 1.169  |  nan  |  nan  | 4.490 |\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 03:52:12 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.132 | book       | 8.074 |\n",
      "\u001B[32m[07/18 03:52:12 d2.utils.events]: \u001B[0m eta: 0:09:06  iter: 899  total_loss: 1.728  loss_cls: 0.677  loss_box_reg: 0.8575  loss_rpn_cls: 0.02204  loss_rpn_loc: 0.1636    time: 7.2992  last_time: 99.1965  data_time: 0.0012  last_data_time: 0.0020   lr: 2.2728e-06  \n",
      "\u001B[32m[07/18 04:09:04 d2.utils.events]: \u001B[0m eta: 0:07:17  iter: 919  total_loss: 1.697  loss_cls: 0.6726  loss_box_reg: 0.8352  loss_rpn_cls: 0.02007  loss_rpn_loc: 0.1707    time: 7.2643  last_time: 5.1653  data_time: 0.0012  last_data_time: 0.0009   lr: 2.3178e-06  \n",
      "\u001B[32m[07/18 04:10:42 d2.utils.events]: \u001B[0m eta: 0:05:27  iter: 939  total_loss: 1.683  loss_cls: 0.6612  loss_box_reg: 0.835  loss_rpn_cls: 0.01548  loss_rpn_loc: 0.162    time: 7.2142  last_time: 4.7301  data_time: 0.0011  last_data_time: 0.0010   lr: 2.3628e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 04:11:29 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 04:11:29 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 04:11:29 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 04:11:29 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 04:11:29 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 04:11:29 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 04:11:42 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 0.9808 s/iter. Eval: 0.0001 s/iter. Total: 0.9812 s/iter. ETA=0:01:06\n",
      "\u001B[32m[07/18 04:11:47 d2.evaluation.evaluator]: \u001B[0mInference done 17/79. Dataloading: 0.0003 s/iter. Inference: 0.9624 s/iter. Eval: 0.0001 s/iter. Total: 0.9629 s/iter. ETA=0:00:59\n",
      "\u001B[32m[07/18 04:11:53 d2.evaluation.evaluator]: \u001B[0mInference done 23/79. Dataloading: 0.0003 s/iter. Inference: 0.9567 s/iter. Eval: 0.0001 s/iter. Total: 0.9573 s/iter. ETA=0:00:53\n",
      "\u001B[32m[07/18 04:11:59 d2.evaluation.evaluator]: \u001B[0mInference done 29/79. Dataloading: 0.0003 s/iter. Inference: 0.9584 s/iter. Eval: 0.0001 s/iter. Total: 0.9590 s/iter. ETA=0:00:47\n",
      "\u001B[32m[07/18 04:12:05 d2.evaluation.evaluator]: \u001B[0mInference done 35/79. Dataloading: 0.0003 s/iter. Inference: 0.9555 s/iter. Eval: 0.0001 s/iter. Total: 0.9561 s/iter. ETA=0:00:42\n",
      "\u001B[32m[07/18 04:12:10 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 0.9587 s/iter. Eval: 0.0001 s/iter. Total: 0.9593 s/iter. ETA=0:00:36\n",
      "\u001B[32m[07/18 04:12:16 d2.evaluation.evaluator]: \u001B[0mInference done 47/79. Dataloading: 0.0004 s/iter. Inference: 0.9593 s/iter. Eval: 0.0001 s/iter. Total: 0.9599 s/iter. ETA=0:00:30\n",
      "\u001B[32m[07/18 04:12:22 d2.evaluation.evaluator]: \u001B[0mInference done 53/79. Dataloading: 0.0003 s/iter. Inference: 0.9601 s/iter. Eval: 0.0001 s/iter. Total: 0.9607 s/iter. ETA=0:00:24\n",
      "\u001B[32m[07/18 04:12:28 d2.evaluation.evaluator]: \u001B[0mInference done 59/79. Dataloading: 0.0003 s/iter. Inference: 0.9629 s/iter. Eval: 0.0001 s/iter. Total: 0.9634 s/iter. ETA=0:00:19\n",
      "\u001B[32m[07/18 04:12:34 d2.evaluation.evaluator]: \u001B[0mInference done 65/79. Dataloading: 0.0003 s/iter. Inference: 0.9626 s/iter. Eval: 0.0001 s/iter. Total: 0.9632 s/iter. ETA=0:00:13\n",
      "\u001B[32m[07/18 04:12:40 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0003 s/iter. Inference: 0.9638 s/iter. Eval: 0.0001 s/iter. Total: 0.9644 s/iter. ETA=0:00:07\n",
      "\u001B[32m[07/18 04:12:45 d2.evaluation.evaluator]: \u001B[0mInference done 77/79. Dataloading: 0.0003 s/iter. Inference: 0.9643 s/iter. Eval: 0.0001 s/iter. Total: 0.9648 s/iter. ETA=0:00:01\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:11.645619 (0.968184 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:11 (0.963531 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.050\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.145\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.055\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.019\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.052\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.235\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.235\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 5.043 | 14.500 | 1.631  |  nan  |  nan  | 5.453 |\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.135 | book       | 9.952 |\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 04:12:48 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 04:12:48 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 04:12:48 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 04:12:48 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 04:12:48 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 04:12:48 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 04:13:01 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.0250 s/iter. Eval: 0.0001 s/iter. Total: 1.0254 s/iter. ETA=0:01:09\n",
      "\u001B[32m[07/18 04:13:06 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0003 s/iter. Inference: 1.0423 s/iter. Eval: 0.0001 s/iter. Total: 1.0429 s/iter. ETA=0:01:05\n",
      "\u001B[32m[07/18 04:13:12 d2.evaluation.evaluator]: \u001B[0mInference done 22/79. Dataloading: 0.0003 s/iter. Inference: 1.0122 s/iter. Eval: 0.0001 s/iter. Total: 1.0128 s/iter. ETA=0:00:57\n",
      "\u001B[32m[07/18 04:13:18 d2.evaluation.evaluator]: \u001B[0mInference done 28/79. Dataloading: 0.0003 s/iter. Inference: 0.9992 s/iter. Eval: 0.0001 s/iter. Total: 0.9998 s/iter. ETA=0:00:50\n",
      "\u001B[32m[07/18 04:13:23 d2.evaluation.evaluator]: \u001B[0mInference done 34/79. Dataloading: 0.0003 s/iter. Inference: 0.9925 s/iter. Eval: 0.0001 s/iter. Total: 0.9931 s/iter. ETA=0:00:44\n",
      "\u001B[32m[07/18 04:13:29 d2.evaluation.evaluator]: \u001B[0mInference done 40/79. Dataloading: 0.0004 s/iter. Inference: 0.9903 s/iter. Eval: 0.0001 s/iter. Total: 0.9909 s/iter. ETA=0:00:38\n",
      "\u001B[32m[07/18 04:13:35 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0004 s/iter. Inference: 0.9890 s/iter. Eval: 0.0001 s/iter. Total: 0.9896 s/iter. ETA=0:00:32\n",
      "\u001B[32m[07/18 04:13:41 d2.evaluation.evaluator]: \u001B[0mInference done 52/79. Dataloading: 0.0004 s/iter. Inference: 0.9874 s/iter. Eval: 0.0001 s/iter. Total: 0.9880 s/iter. ETA=0:00:26\n",
      "\u001B[32m[07/18 04:13:47 d2.evaluation.evaluator]: \u001B[0mInference done 58/79. Dataloading: 0.0004 s/iter. Inference: 0.9890 s/iter. Eval: 0.0001 s/iter. Total: 0.9896 s/iter. ETA=0:00:20\n",
      "\u001B[32m[07/18 04:13:53 d2.evaluation.evaluator]: \u001B[0mInference done 64/79. Dataloading: 0.0004 s/iter. Inference: 0.9870 s/iter. Eval: 0.0001 s/iter. Total: 0.9876 s/iter. ETA=0:00:14\n",
      "\u001B[32m[07/18 04:13:59 d2.evaluation.evaluator]: \u001B[0mInference done 70/79. Dataloading: 0.0004 s/iter. Inference: 0.9868 s/iter. Eval: 0.0001 s/iter. Total: 0.9874 s/iter. ETA=0:00:08\n",
      "\u001B[32m[07/18 04:14:05 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0004 s/iter. Inference: 0.9865 s/iter. Eval: 0.0001 s/iter. Total: 0.9871 s/iter. ETA=0:00:02\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:13.305538 (0.990615 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:12 (0.985984 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.050\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.145\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.055\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.019\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.052\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.235\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.235\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 5.043 | 14.500 | 1.631  |  nan  |  nan  | 5.453 |\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 04:14:08 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP    |\n",
      "|:-----------|:------|:-----------|:------|\n",
      "| person     | 0.135 | book       | 9.952 |\n",
      "\u001B[32m[07/18 04:14:57 d2.utils.events]: \u001B[0m eta: 0:03:37  iter: 959  total_loss: 1.689  loss_cls: 0.6602  loss_box_reg: 0.8534  loss_rpn_cls: 0.01753  loss_rpn_loc: 0.1645    time: 7.2459  last_time: 5.0366  data_time: 0.0010  last_data_time: 0.0013   lr: 2.4078e-06  \n",
      "\u001B[32m[07/18 04:16:31 d2.utils.events]: \u001B[0m eta: 0:01:48  iter: 979  total_loss: 1.67  loss_cls: 0.6571  loss_box_reg: 0.8273  loss_rpn_cls: 0.02235  loss_rpn_loc: 0.1647    time: 7.1940  last_time: 4.0703  data_time: 0.0009  last_data_time: 0.0008   lr: 2.4528e-06  \n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 04:18:10 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 04:18:10 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 04:18:10 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 04:18:10 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 04:18:10 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 04:18:10 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 04:18:23 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.0290 s/iter. Eval: 0.0001 s/iter. Total: 1.0294 s/iter. ETA=0:01:10\n",
      "\u001B[32m[07/18 04:18:28 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0003 s/iter. Inference: 1.0219 s/iter. Eval: 0.0001 s/iter. Total: 1.0224 s/iter. ETA=0:01:04\n",
      "\u001B[32m[07/18 04:18:33 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0003 s/iter. Inference: 1.0164 s/iter. Eval: 0.0001 s/iter. Total: 1.0170 s/iter. ETA=0:00:58\n",
      "\u001B[32m[07/18 04:18:38 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0003 s/iter. Inference: 1.0159 s/iter. Eval: 0.0001 s/iter. Total: 1.0164 s/iter. ETA=0:00:53\n",
      "\u001B[32m[07/18 04:18:43 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0003 s/iter. Inference: 1.0149 s/iter. Eval: 0.0001 s/iter. Total: 1.0154 s/iter. ETA=0:00:48\n",
      "\u001B[32m[07/18 04:18:48 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0004 s/iter. Inference: 1.0194 s/iter. Eval: 0.0001 s/iter. Total: 1.0200 s/iter. ETA=0:00:43\n",
      "\u001B[32m[07/18 04:18:54 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 1.0216 s/iter. Eval: 0.0001 s/iter. Total: 1.0223 s/iter. ETA=0:00:38\n",
      "\u001B[32m[07/18 04:18:59 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0004 s/iter. Inference: 1.0224 s/iter. Eval: 0.0001 s/iter. Total: 1.0230 s/iter. ETA=0:00:33\n",
      "\u001B[32m[07/18 04:19:04 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0004 s/iter. Inference: 1.0220 s/iter. Eval: 0.0001 s/iter. Total: 1.0226 s/iter. ETA=0:00:28\n",
      "\u001B[32m[07/18 04:19:09 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0004 s/iter. Inference: 1.0222 s/iter. Eval: 0.0001 s/iter. Total: 1.0229 s/iter. ETA=0:00:23\n",
      "\u001B[32m[07/18 04:19:14 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0004 s/iter. Inference: 1.0231 s/iter. Eval: 0.0001 s/iter. Total: 1.0237 s/iter. ETA=0:00:18\n",
      "\u001B[32m[07/18 04:19:19 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0004 s/iter. Inference: 1.0236 s/iter. Eval: 0.0001 s/iter. Total: 1.0243 s/iter. ETA=0:00:13\n",
      "\u001B[32m[07/18 04:19:25 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0004 s/iter. Inference: 1.0254 s/iter. Eval: 0.0001 s/iter. Total: 1.0261 s/iter. ETA=0:00:08\n",
      "\u001B[32m[07/18 04:19:30 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0004 s/iter. Inference: 1.0257 s/iter. Eval: 0.0001 s/iter. Total: 1.0263 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:16.225162 (1.030070 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:15 (1.025231 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.167\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.064\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.013\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.050\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.239\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.239\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 5.966 | 16.672 | 2.119  |  nan  |  nan  | 6.424 |\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP     |\n",
      "|:-----------|:------|:-----------|:-------|\n",
      "| person     | 0.043 | book       | 11.888 |\n",
      "\u001B[32m[07/18 04:19:33 d2.utils.events]: \u001B[0m eta: 0:00:00  iter: 999  total_loss: 1.682  loss_cls: 0.6477  loss_box_reg: 0.8452  loss_rpn_cls: 0.01426  loss_rpn_loc: 0.1654    time: 7.2322  last_time: 88.7368  data_time: 0.0009  last_data_time: 0.0011   lr: 2.4978e-06  \n",
      "\u001B[32m[07/18 04:19:33 d2.engine.hooks]: \u001B[0mOverall training speed: 998 iterations in 2:00:17 (7.2322 s / it)\n",
      "\u001B[32m[07/18 04:19:33 d2.engine.hooks]: \u001B[0mTotal training time: 2:31:15 (0:30:57 on hooks)\n",
      "\u001B[5m\u001B[31mWARNING\u001B[0m \u001B[32m[07/18 04:19:33 d2.evaluation.coco_evaluation]: \u001B[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001B[32m[07/18 04:19:33 d2.data.dataset_mapper]: \u001B[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001B[32m[07/18 04:19:33 d2.data.common]: \u001B[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001B[32m[07/18 04:19:33 d2.data.common]: \u001B[0mSerializing 79 elements to byte tensors and concatenating them all ...\n",
      "\u001B[32m[07/18 04:19:33 d2.data.common]: \u001B[0mSerialized dataset takes 0.11 MiB\n",
      "\u001B[32m[07/18 04:19:33 d2.evaluation.evaluator]: \u001B[0mStart inference on 79 batches\n",
      "\u001B[32m[07/18 04:19:47 d2.evaluation.evaluator]: \u001B[0mInference done 11/79. Dataloading: 0.0003 s/iter. Inference: 1.0472 s/iter. Eval: 0.0001 s/iter. Total: 1.0477 s/iter. ETA=0:01:11\n",
      "\u001B[32m[07/18 04:19:53 d2.evaluation.evaluator]: \u001B[0mInference done 16/79. Dataloading: 0.0003 s/iter. Inference: 1.0826 s/iter. Eval: 0.0001 s/iter. Total: 1.0831 s/iter. ETA=0:01:08\n",
      "\u001B[32m[07/18 04:19:58 d2.evaluation.evaluator]: \u001B[0mInference done 21/79. Dataloading: 0.0003 s/iter. Inference: 1.0712 s/iter. Eval: 0.0001 s/iter. Total: 1.0718 s/iter. ETA=0:01:02\n",
      "\u001B[32m[07/18 04:20:03 d2.evaluation.evaluator]: \u001B[0mInference done 26/79. Dataloading: 0.0003 s/iter. Inference: 1.0679 s/iter. Eval: 0.0001 s/iter. Total: 1.0685 s/iter. ETA=0:00:56\n",
      "\u001B[32m[07/18 04:20:09 d2.evaluation.evaluator]: \u001B[0mInference done 31/79. Dataloading: 0.0004 s/iter. Inference: 1.0650 s/iter. Eval: 0.0001 s/iter. Total: 1.0656 s/iter. ETA=0:00:51\n",
      "\u001B[32m[07/18 04:20:14 d2.evaluation.evaluator]: \u001B[0mInference done 36/79. Dataloading: 0.0004 s/iter. Inference: 1.0631 s/iter. Eval: 0.0001 s/iter. Total: 1.0637 s/iter. ETA=0:00:45\n",
      "\u001B[32m[07/18 04:20:19 d2.evaluation.evaluator]: \u001B[0mInference done 41/79. Dataloading: 0.0004 s/iter. Inference: 1.0636 s/iter. Eval: 0.0001 s/iter. Total: 1.0642 s/iter. ETA=0:00:40\n",
      "\u001B[32m[07/18 04:20:25 d2.evaluation.evaluator]: \u001B[0mInference done 46/79. Dataloading: 0.0004 s/iter. Inference: 1.0638 s/iter. Eval: 0.0001 s/iter. Total: 1.0644 s/iter. ETA=0:00:35\n",
      "\u001B[32m[07/18 04:20:30 d2.evaluation.evaluator]: \u001B[0mInference done 51/79. Dataloading: 0.0004 s/iter. Inference: 1.0632 s/iter. Eval: 0.0001 s/iter. Total: 1.0638 s/iter. ETA=0:00:29\n",
      "\u001B[32m[07/18 04:20:35 d2.evaluation.evaluator]: \u001B[0mInference done 56/79. Dataloading: 0.0004 s/iter. Inference: 1.0645 s/iter. Eval: 0.0001 s/iter. Total: 1.0651 s/iter. ETA=0:00:24\n",
      "\u001B[32m[07/18 04:20:41 d2.evaluation.evaluator]: \u001B[0mInference done 61/79. Dataloading: 0.0004 s/iter. Inference: 1.0649 s/iter. Eval: 0.0001 s/iter. Total: 1.0655 s/iter. ETA=0:00:19\n",
      "\u001B[32m[07/18 04:20:46 d2.evaluation.evaluator]: \u001B[0mInference done 66/79. Dataloading: 0.0004 s/iter. Inference: 1.0636 s/iter. Eval: 0.0001 s/iter. Total: 1.0642 s/iter. ETA=0:00:13\n",
      "\u001B[32m[07/18 04:20:51 d2.evaluation.evaluator]: \u001B[0mInference done 71/79. Dataloading: 0.0004 s/iter. Inference: 1.0639 s/iter. Eval: 0.0001 s/iter. Total: 1.0646 s/iter. ETA=0:00:08\n",
      "\u001B[32m[07/18 04:20:57 d2.evaluation.evaluator]: \u001B[0mInference done 76/79. Dataloading: 0.0004 s/iter. Inference: 1.0635 s/iter. Eval: 0.0001 s/iter. Total: 1.0642 s/iter. ETA=0:00:03\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.evaluator]: \u001B[0mTotal inference time: 0:01:19.000539 (1.067575 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.evaluator]: \u001B[0mTotal inference pure compute time: 0:01:18 (1.062700 s / iter per device, on 1 devices)\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.coco_evaluation]: \u001B[0mPreparing results for COCO format ...\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.coco_evaluation]: \u001B[0mSaving results to ./output/inference/coco_instances_results.json\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.coco_evaluation]: \u001B[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.fast_eval_api]: \u001B[0mEvaluate annotation type *bbox*\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.fast_eval_api]: \u001B[0mAccumulating evaluation results...\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.fast_eval_api]: \u001B[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.167\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.064\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.013\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.050\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.239\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.239\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.coco_evaluation]: \u001B[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 5.966 | 16.672 | 2.119  |  nan  |  nan  | 6.424 |\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.coco_evaluation]: \u001B[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001B[32m[07/18 04:21:00 d2.evaluation.coco_evaluation]: \u001B[0mPer-category bbox AP: \n",
      "| category   | AP    | category   | AP     |\n",
      "|:-----------|:------|:-----------|:-------|\n",
      "| person     | 0.043 | book       | 11.888 |\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Inference",
   "id": "8a82c5e80cfaccd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T17:39:14.524387Z",
     "start_time": "2024-07-18T16:54:13.533299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from research.detectron2.detectron2 import model_zoo\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# Load the configuration and set the trained weights\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TEST = (\"custom_val\",)\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # Path to the trained model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # Ensure this matches the number of classes (person and book)\n",
    "cfg.MODEL.DEVICE = \"cpu\"  # Use CPU for inference\n",
    "\n",
    "# Setup the predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Provide the path to your input video\n",
    "video_path = './vids/20240617_210932.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "output_path = './output_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "# Metadata for visualizer\n",
    "custom_metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    outputs = predictor(frame)\n",
    "    v = Visualizer(frame[:, :, ::-1], metadata=custom_metadata, instance_mode=ColorMode.IMAGE)  # Use ColorMode.IMAGE for color output\n",
    "    out_frame = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "    out.write(out_frame.get_image()[:, :, ::-1])\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Output video saved to {output_path}\")\n"
   ],
   "id": "6de588f32f9a2487",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[07/18 12:54:13 d2.checkpoint.detection_checkpoint]: \u001B[0m[DetectionCheckpointer] Loading from ./output/model_final.pth ...\n",
      "Output video saved to ./output_video.mp4\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "96bc824e4ecae5ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
